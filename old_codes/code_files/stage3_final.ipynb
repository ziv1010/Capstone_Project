{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70a003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT : /scratch/ziv_baretto/llmserve\n",
      "DATA_DIR     : /scratch/ziv_baretto/llmserve/data\n",
      "STAGE2_OUT   : /scratch/ziv_baretto/llmserve/stage2_out\n",
      "STAGE3_OUT   : /scratch/ziv_baretto/llmserve/stage3_out\n",
      "\n",
      "ğŸ¯ Starting Stage 3 for: TSK-002\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ STAGE 3: Planning for TSK-002\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ’» [SYSTEM]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "You are a data pipeline planning agent.\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "CRITICAL RULES\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "1. You MUST end by calling save_stage3_plan(plan_json=...) - this is YOUR ONLY SUCCESS CRITERIA\n",
      "2. NEVER write JSON in your reasoning - build it silently and pass to the tool\n",
      "3. Be dataset-agnostic - no domain assumptions\n",
      "4. Set plan_id EXACTLY to \"PLAN-{selected_task_id}\"\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ‘¤ [USER]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Create Stage 3 plan for 'TSK-002'.\n",
      "\n",
      "Steps:\n",
      "1. load_task_proposal('TSK-002')\n",
      "2. list_data_files()\n",
      "3. inspect_data_file() for required files\n",
      "4. Build Stage3Plan JSON silently\n",
      "5. save_stage3_plan(plan_json=...) â† MANDATORY FINAL STEP\n",
      "\n",
      "Rules:\n",
      "- Set plan_id = 'PLAN-TSK-002'\n",
      "- Never print JSON in reasoning\n",
      "- Your ONLY success criteria: call save_stage3_plan()\n",
      "- Complete in 3-4 rounds\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 1]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ load_task_proposal\n",
      "     task_id: TSK-002\n",
      "     [id: chatcmpl-tool-c878b7a5b8474950b3a9a9b176b7b03c]\n",
      "\n",
      "  ğŸ“Œ list_data_files\n",
      "     [id: chatcmpl-tool-6e66ceff6e76495da6c70240ae3c7334]\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: load_task_proposal]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"id\": \"TSK-002\",\n",
      "  \"category\": \"descriptive\",\n",
      "  \"title\": \"Rice Export Trends\",\n",
      "  \"problem_statement\": \"Analyze the trends and patterns in rice export quantities and values over time to understand the dynamics of the rice trade.\",\n",
      "  \"required_files\": [\n",
      "    \"Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv\"\n",
      "  ],\n",
      "  \"join_plan\": {\n",
      "    \"hypothesized_keys\": [],\n",
      "    \"notes\": \"No join required for this task.\"\n",
      "  },\n",
      "  \"target\": null,\n",
      "  \"feature_plan\": {\n",
      "    \"candidates\": [\n",
      "      \"Quantity\",\n",
      "      \"Value\"\n",
      "    ],\n",
      "    \"transform_ideas\": [\n",
      "      \"rolling averages\",\n",
      "      \"seasonal decomposition\"\n",
      "    ],\n",
      "    \"handling_missingness\": \"Ignore missing values for this task.\"\n",
      "  },\n",
      "  \"validation_plan\": \"Verify that the results make sense and are consistent with prior knowledge.\",\n",
      "  \"quality_checks\": [\n",
      "    \"Check that the data is correctly loaded and there are no errors.\",\n",
      "    \"Verify that the results are not affected by outliers.\"\n",
      "  ],\n",
      "  \"expected_outputs\": [\n",
      "    \"summary tables\",\n",
      "    \"plots of trends and patterns\",\n",
      "    \"summary report\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: list_data_files]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[\"All-India-Estimates-of-Area,-Production-&-Yield-of-Food-Grains.csv\", \"All-India-profile_-Crop-wise-Area.csv\", \"Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv\"]\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 2]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ inspect_data_file\n",
      "     filename: Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv\n",
      "     n_rows: 10\n",
      "     [id: chatcmpl-tool-b5b77f06165a47348cfb281d752f4faa]\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: inspect_data_file]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "=== FILE: Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv ===\n",
      "Shape: (8, 23)\n",
      "\n",
      "Columns (23):\n",
      "  - HS Code: object (nulls: 0)\n",
      "  - Description: object (nulls: 0)\n",
      "  - 2018 - 19-Quantity: float64 (nulls: 1)\n",
      "  - 2018 - 19-Value (INR): float64 (nulls: 1)\n",
      "  - 2018 - 19-Value (USD): float64 (nulls: 1)\n",
      "  - 2019 - 20-Quantity: float64 (nulls: 1)\n",
      "  - 2019 - 20-Value (INR): float64 (nulls: 1)\n",
      "  - 2019 - 20-Value (USD): float64 (nulls: 1)\n",
      "  - 2020 - 21-Quantity: float64 (nulls: 0)\n",
      "  - 2020 - 21-Value (INR): float64 (nulls: 0)\n",
      "  - 2020 - 21-Value (USD): float64 (nulls: 0)\n",
      "  - 2021 - 22-Quantity: float64 (nulls: 0)\n",
      "  - 2021 - 22-Value (INR): float64 (nulls: 0)\n",
      "  - 2021 - 22-Value (USD): float64 (nulls: 0)\n",
      "  - 2022 - 23-Quantity: float64 (nulls: 2)\n",
      "  - 2022 - 23-Value (INR): float64 (nulls: 2)\n",
      "  - 2022 - 23-Value (USD): float64 (nulls: 2)\n",
      "  - 2023 - 24-Quantity: float64 (nulls: 5)\n",
      "  - 2023 - 24-Value (INR): float64 (nulls: 5)\n",
      "  - 2023 - 24-Value (USD): float64 (nulls: 5)\n",
      "  - 2024 - 25-Quantity: float64 (nulls: 2)\n",
      "  - 2024 - 25-Value (INR): float64 (nulls: 2)\n",
      "  - 2024 - 25-Value (USD): float64 (nulls: 2)\n",
      "\n",
      "First 8 rows:\n",
      "    HS Code                                Description  2018 - 19-Quantity  2018 - 19-Value (INR)  2018 - 19-Value (USD)  2019 - 20-Quantity  2019 - 20-Value (INR)  2019 - 20-Value (USD)  2020 - 21-Quantity  2020 - 21-Value (INR)  2020 - 21-Value (USD)  2021 - 22-Quantity  2021 - 22-Value (INR)  2021 - 22-Value (USD)  2022 - 23-Quantity  2022 - 23-Value (INR)  2022 - 23-Value (USD)  2023 - 24-Quantity  2023 - 24-Value (INR)  2023 - 24-Value (USD)  2024 - 25-Quantity  2024 - 25-Value (INR)  2024 - 25-Value (USD)\n",
      "...[truncated]...\n",
      "    HS Code                                Description  2018 - 19-Quantity  2018 - 19-Value (INR)  2018 - 19-Value (USD)  2019 - 20-Quantity  2019 - 20-Value (INR)  2019 - 20-Value (USD)  2020 - 21-Quantity  2020 - 21-Value (INR)  2020 - 21-Value (USD)  2021 - 22-Quantity  2021 - 22-Value (INR)  2021 - 22-Value (USD)  2022 - 23-Quantity  2022 - 23-Value (INR)  2022 - 23-Value (USD)  2023 - 24-Quantity  2023 - 24-Value (INR)  2023 - 24-Value (USD)  2024 - 25-Quantity  2024 - 25-Value (INR)  2024 - 25-Value (USD)\n",
      "0  10064000                                BROKEN RICE                5.78               93879.80                1412.71                0.13                2950.51                  40.83                8.11              132478.35                1819.38               15.81              287288.98                3873.44                 NaN                    NaN                    NaN                 NaN                    NaN                    NaN                 NaN                    NaN                    NaN\n",
      "1  10063020                               BASMATI RICE                2.28              206428.48                2961.17                2.37              198775.15                2800.32                3.44              257464.54                3492.03                5.60              451426.53                6048.15                6.28              597687.02                7419.55                6.32              634508.97                7678.43                1.72              173189.16                2051.46\n",
      "2  10061010                  RICE IN HUSK OF SEED QLTY                4.23              661071.29                9445.18                3.85              590893.17                8309.23                5.00              807452.28               10829.89                4.30              669925.30                9088.67                3.66              581421.73                7373.64                3.95              677520.08                8189.88                2.60              481168.99                5749.40\n",
      "3  10061090                          OTHR RICE IN HUSK                0.70               85817.65                1186.75                0.08               12765.29                 178.95                0.09               16720.16                 227.54                0.63               15020.00                 204.18                0.00                   1.51                   0.02                 NaN                    NaN                    NaN                0.06               13859.22                 165.37\n",
      "4  10063090  RICE EXCPTG PARBOILED (EXCL BASMATI RICE)              385.02            10304788.47              152707.99                2.77               81734.92                1164.58               93.89             2527526.48               34715.42              117.69             3158607.47               42486.87              111.48             2517228.91               30987.44                 NaN                    NaN                    NaN               56.51             2072824.22               24053.20\n",
      "5  10063010                             RICE PARBOILED              134.23             3721867.54               54595.36                6.25              168842.30                2421.53              785.76            21482122.60              294972.84             1484.82            41263291.83              556110.97              725.30            21781192.00              267576.33                 NaN                    NaN                    NaN              750.62            28373214.53              328851.11\n",
      "6  10062000                        HUSKED (BROWN) RICE                 NaN                    NaN                    NaN                 NaN                    NaN                    NaN               21.18              615976.55                8403.14                5.73              160072.91                2184.77                 NaN                    NaN                    NaN                 NaN                    NaN                    NaN                 NaN                    NaN                    NaN\n",
      "7     Total                                      Total              532.23            15073853.24              222309.16               15.46             1055961.35               14915.44              917.45            25839740.96              354460.24             1634.59            46005633.03              619997.04              846.72            25477531.17              313356.99               10.28             1312029.06               15868.31              811.51            31114256.12              360870.53\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 3]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ save_stage3_plan\n",
      "     plan_json: {\"plan_id\": \"PLAN-TSK-002\", \"selected_task_id\": \"TSK-002\", \"goal\": \"Analyze the trends and patterns ...[truncated]...ics\": [\"Summary Statistics\"], \"notes\": [\"No join required for this task.\"], \"key_normalization\": []}\n",
      "     [id: chatcmpl-tool-c87251915aac4318914c461cd225b9a6]\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: save_stage3_plan]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Plan saved successfully to: /scratch/ziv_baretto/llmserve/stage3_out/PLAN-TSK-002.json\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 4]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "The Stage 3 plan for task 'TSK-002' has been successfully saved. The plan includes the analysis of rice export trends over multiple years, with specific focus on quantities and values. The plan was constructed according to the provided guidelines and saved as 'PLAN-TSK-002'.\n",
      "\n",
      "================================================================================\n",
      "âœ… Complete - 4 rounds\n",
      "================================================================================\n",
      "\n",
      "âœ… SUCCESS! Plan saved to: /scratch/ziv_baretto/llmserve/stage3_out/PLAN-TSK-002.json\n",
      "\n",
      "Plan Summary:\n",
      "  Goal: Analyze the trends and patterns in rice export quantities and values over time to understand the dynamics of the rice trade.\n",
      "  Files: 1\n",
      "  Joins: 1\n",
      "  Features: 3\n",
      "  Columns: 23\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal, Union\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# langchain / langgraph imports\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ===========================\n",
    "# Paths\n",
    "# ===========================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/scratch/ziv_baretto/llmserve\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "STAGE2_OUT_DIR = PROJECT_ROOT / \"stage2_out\"\n",
    "STAGE3_OUT_DIR = PROJECT_ROOT / \"stage3_out\"\n",
    "STAGE3_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT :\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR     :\", DATA_DIR)\n",
    "print(\"STAGE2_OUT   :\", STAGE2_OUT_DIR)\n",
    "print(\"STAGE3_OUT   :\", STAGE3_OUT_DIR)\n",
    "\n",
    "# ===========================\n",
    "# Stage 2 models\n",
    "# ===========================\n",
    "\n",
    "TaskCategory = Literal[\"predictive\", \"descriptive\", \"unsupervised\"]\n",
    "\n",
    "\n",
    "class JoinPlan(BaseModel):\n",
    "    hypothesized_keys: List[List[str]] = Field(default_factory=list)\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "\n",
    "class TargetSpec(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "    granularity: Optional[List[str]] = None\n",
    "    horizon: Optional[str] = None\n",
    "\n",
    "\n",
    "class FeaturePlan(BaseModel):\n",
    "    candidates: List[str] = Field(default_factory=list)\n",
    "    transform_ideas: List[str] = Field(default_factory=list)\n",
    "    handling_missingness: Optional[str] = None\n",
    "\n",
    "\n",
    "class TaskProposal(BaseModel):\n",
    "    id: str\n",
    "    category: TaskCategory\n",
    "    title: str\n",
    "    problem_statement: str\n",
    "    required_files: List[str] = Field(default_factory=list)\n",
    "    join_plan: JoinPlan = Field(default_factory=JoinPlan)\n",
    "    target: Optional[TargetSpec] = None\n",
    "    feature_plan: FeaturePlan = Field(default_factory=FeaturePlan)\n",
    "    validation_plan: Optional[str] = None\n",
    "    quality_checks: List[str] = Field(default_factory=list)\n",
    "    expected_outputs: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class Stage2Output(BaseModel):\n",
    "    proposals: List[TaskProposal]\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Stage 3 Plan Models\n",
    "# ===========================\n",
    "\n",
    "\n",
    "class ArtifactSpec(BaseModel):\n",
    "    intermediate_table: str = Field(description=\"Filename for the output table\")\n",
    "    intermediate_format: Literal[\"parquet\", \"csv\", \"feather\"] = \"parquet\"\n",
    "    expected_columns: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of all columns expected in final table\"\n",
    "    )\n",
    "    expected_row_count_range: Optional[tuple[int, int]] = Field(\n",
    "        default=None,\n",
    "        description=\"Expected min/max row count\"\n",
    "    )\n",
    "\n",
    "\n",
    "class KeyNormalization(BaseModel):\n",
    "    column_name: str\n",
    "    mapping: Dict[str, str] = Field(default_factory=dict)\n",
    "    format_type: Optional[str] = None\n",
    "    valid_range: Optional[tuple[Any, Any]] = None\n",
    "\n",
    "\n",
    "class FileInstruction(BaseModel):\n",
    "    file_id: str\n",
    "    original_name: str\n",
    "    alias: str\n",
    "    filters: List[str] = Field(default_factory=list)\n",
    "    rename_columns: Dict[str, str] = Field(default_factory=dict)\n",
    "    join_keys: List[str] = Field(default_factory=list)\n",
    "    keep_columns: List[str] = Field(default_factory=list)\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "\n",
    "class JoinValidation(BaseModel):\n",
    "    check_duplicates_on_keys: List[str] = Field(default_factory=list)\n",
    "    expected_unique: bool = False\n",
    "    check_row_count_stable: bool = False\n",
    "    check_no_duplicates_introduced: bool = False\n",
    "    acceptable_coverage: Optional[float] = None\n",
    "    max_cardinality_ratio: Optional[float] = None\n",
    "\n",
    "\n",
    "class JoinStep(BaseModel):\n",
    "    step: int\n",
    "    description: str\n",
    "    left_table: str\n",
    "    right_table: Optional[str] = None\n",
    "    join_type: Literal[\"base\", \"inner\", \"left\", \"right\", \"outer\"] = \"base\"\n",
    "    join_keys: List[str] = Field(default_factory=list)\n",
    "    expected_cardinality: str = \"base\"\n",
    "    validation: JoinValidation = Field(default_factory=JoinValidation)\n",
    "\n",
    "\n",
    "class FeatureEngineering(BaseModel):\n",
    "    feature_name: str\n",
    "    description: str\n",
    "    transform: str\n",
    "    depends_on: List[str] = Field(default_factory=list)\n",
    "    implementation: Optional[str] = None\n",
    "\n",
    "\n",
    "class TimeSplit(BaseModel):\n",
    "    method: Literal[\"year-based\", \"date-based\", \"rolling-window\", \"none\"] = \"none\"\n",
    "    train_years: Optional[str] = None\n",
    "    test_years: Optional[str] = None\n",
    "    validation_years: Optional[str] = None\n",
    "    leakage_check: str = \"Not applicable\"\n",
    "\n",
    "\n",
    "class CoverageCheck(BaseModel):\n",
    "    check: str\n",
    "    threshold: float\n",
    "    description: str\n",
    "    action_if_violation: Optional[str] = None\n",
    "\n",
    "\n",
    "class CardinalityCheck(BaseModel):\n",
    "    check: str\n",
    "    expected: str\n",
    "    action_if_violation: str\n",
    "\n",
    "\n",
    "class ValidationSpec(BaseModel):\n",
    "    time_split: Optional[TimeSplit] = Field(default_factory=lambda: TimeSplit())\n",
    "    coverage_checks: List[CoverageCheck] = Field(default_factory=list)\n",
    "    cardinality_checks: List[CardinalityCheck] = Field(default_factory=list)\n",
    "    additional_checks: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class Stage3Plan(BaseModel):\n",
    "    # Metadata\n",
    "    plan_id: str\n",
    "    selected_task_id: str\n",
    "    goal: str\n",
    "    task_category: TaskCategory\n",
    "    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "    created_by: str = Field(default=\"stage3_agent\")\n",
    "    \n",
    "    # Output specification\n",
    "    artifacts: ArtifactSpec\n",
    "    \n",
    "    # Key normalization\n",
    "    key_normalization: List[KeyNormalization] = Field(default_factory=list)\n",
    "    \n",
    "    # File loading instructions\n",
    "    file_instructions: List[FileInstruction] = Field(default_factory=list)\n",
    "    \n",
    "    # Join strategy\n",
    "    join_steps: List[JoinStep] = Field(default_factory=list)\n",
    "    \n",
    "    # Feature engineering\n",
    "    feature_engineering: List[FeatureEngineering] = Field(default_factory=list)\n",
    "    \n",
    "    # Validation strategy\n",
    "    validation: ValidationSpec = Field(default_factory=ValidationSpec)\n",
    "    \n",
    "    # Expected models and metrics\n",
    "    expected_model_types: List[str] = Field(default_factory=list)\n",
    "    evaluation_metrics: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    # Documentation\n",
    "    notes: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Helper Functions (actual implementations)\n",
    "# ===========================\n",
    "\n",
    "def _load_task_proposal_impl(task_id: str) -> str:\n",
    "    \"\"\"Load a single TaskProposal by ID.\"\"\"\n",
    "    path = STAGE2_OUT_DIR / \"task_proposals.json\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Could not find task_proposals.json in {STAGE2_OUT_DIR}\")\n",
    "\n",
    "    raw = path.read_text()\n",
    "    data = json.loads(raw)\n",
    "    stage2 = Stage2Output.model_validate(data)\n",
    "    for p in stage2.proposals:\n",
    "        if p.id == task_id:\n",
    "            return p.model_dump_json(indent=2)\n",
    "    raise ValueError(f\"No TaskProposal with id={task_id!r} found.\")\n",
    "\n",
    "\n",
    "def _list_data_files_impl() -> List[str]:\n",
    "    \"\"\"List available data files.\"\"\"\n",
    "    exts = {\".csv\", \".tsv\", \".parquet\", \".feather\"}\n",
    "    files = [p.name for p in DATA_DIR.iterdir() if p.suffix.lower() in exts]\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def _inspect_data_file_impl(filename: str, n_rows: int = 10) -> str:\n",
    "    \"\"\"Inspect a data file - shows head, dtypes, nulls.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    path = DATA_DIR / filename\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No such file: {filename}\")\n",
    "\n",
    "    if path.suffix.lower() == \".parquet\":\n",
    "        df = pd.read_parquet(path)\n",
    "    elif path.suffix.lower() == \".feather\":\n",
    "        df = pd.read_feather(path)\n",
    "    else:\n",
    "        df = pd.read_csv(path, nrows=max(n_rows, 100))\n",
    "\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf):\n",
    "        print(f\"=== FILE: {filename} ===\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  - {col}: {df[col].dtype} (nulls: {df[col].isnull().sum()})\")\n",
    "        print(f\"\\nFirst {min(n_rows, len(df))} rows:\")\n",
    "        print(df.head(n_rows).to_string())\n",
    "    return buf.getvalue()\n",
    "\n",
    "\n",
    "def _python_sandbox_impl(code: str) -> str:\n",
    "    \"\"\"Execute Python code for data exploration.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    def load_dataframe(filename: str, n_rows: Optional[int] = None):\n",
    "        path = DATA_DIR / filename\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"No such file: {filename}\")\n",
    "        if path.suffix.lower() == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif path.suffix.lower() == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        else:\n",
    "            if n_rows:\n",
    "                return pd.read_csv(path, nrows=n_rows)\n",
    "            return pd.read_csv(path)\n",
    "    \n",
    "    globals_dict = {\n",
    "        \"__name__\": \"__stage3_sandbox__\",\n",
    "        \"pd\": pd,\n",
    "        \"json\": json,\n",
    "        \"Path\": Path,\n",
    "        \"DATA_DIR\": DATA_DIR,\n",
    "        \"load_dataframe\": load_dataframe,\n",
    "    }\n",
    "    \n",
    "    local_env = {}\n",
    "    buf = io.StringIO()\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(buf):\n",
    "            exec(code, globals_dict, local_env)\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {e}\"\n",
    "    return buf.getvalue() or \"[No output]\"\n",
    "\n",
    "\n",
    "def _save_stage3_plan_impl(plan_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Validate and save a Stage3Plan.\n",
    "    \n",
    "    Args:\n",
    "        plan_json: Complete JSON string of Stage3Plan\n",
    "        \n",
    "    Returns:\n",
    "        Success message with path, or raises ValueError\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Parse JSON\n",
    "    try:\n",
    "        raw_obj = json.loads(plan_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON: {e}\") from e\n",
    "\n",
    "    # Schema validation\n",
    "    try:\n",
    "        plan = Stage3Plan.model_validate(raw_obj)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Schema validation failed: {e}\") from e\n",
    "\n",
    "    # Validate files exist (use actual function)\n",
    "    available_files = set(_list_data_files_impl())\n",
    "    for fi in plan.file_instructions:\n",
    "        if fi.original_name not in available_files:\n",
    "            raise ValueError(f\"File {fi.original_name!r} not found in DATA_DIR\")\n",
    "\n",
    "    # For join steps, do basic validation\n",
    "    if plan.join_steps:\n",
    "        file_cache = {}\n",
    "        \n",
    "        # Load files\n",
    "        for fi in plan.file_instructions:\n",
    "            try:\n",
    "                path = DATA_DIR / fi.original_name\n",
    "                if path.suffix.lower() == \".parquet\":\n",
    "                    df = pd.read_parquet(path)\n",
    "                else:\n",
    "                    df = pd.read_csv(path, nrows=100)\n",
    "                file_cache[fi.alias] = df\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to load {fi.original_name}: {e}\") from e\n",
    "        \n",
    "        # Validate joins\n",
    "        for idx, js in enumerate(plan.join_steps):\n",
    "            if js.join_type == \"base\":\n",
    "                if js.left_table not in file_cache:\n",
    "                    raise ValueError(f\"Base table {js.left_table!r} not found\")\n",
    "                continue\n",
    "                \n",
    "            if js.left_table not in file_cache:\n",
    "                raise ValueError(f\"Left table {js.left_table!r} not found\")\n",
    "            if js.right_table and js.right_table not in file_cache:\n",
    "                raise ValueError(f\"Right table {js.right_table!r} not found\")\n",
    "            \n",
    "            # Check keys exist\n",
    "            df_left = file_cache[js.left_table]\n",
    "            df_right = file_cache.get(js.right_table) if js.right_table else None\n",
    "            \n",
    "            missing_left = [k for k in js.join_keys if k not in df_left.columns]\n",
    "            if missing_left:\n",
    "                raise ValueError(f\"Join {idx}: keys {missing_left} missing in {js.left_table}\")\n",
    "            \n",
    "            if df_right is not None:\n",
    "                missing_right = [k for k in js.join_keys if k not in df_right.columns]\n",
    "                if missing_right:\n",
    "                    raise ValueError(f\"Join {idx}: keys {missing_right} missing in {js.right_table}\")\n",
    "\n",
    "    # Save\n",
    "    out_path = STAGE3_OUT_DIR / f\"{plan.plan_id}.json\"\n",
    "    out_path.write_text(plan.model_dump_json(indent=2))\n",
    "    \n",
    "    return f\"âœ… Plan saved successfully to: {out_path}\"\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Tools (wrapping the implementations)\n",
    "# ===========================\n",
    "\n",
    "@tool\n",
    "def load_task_proposal(task_id: str) -> str:\n",
    "    \"\"\"Load a single TaskProposal by ID.\"\"\"\n",
    "    return _load_task_proposal_impl(task_id)\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_data_files() -> List[str]:\n",
    "    \"\"\"List available data files.\"\"\"\n",
    "    return _list_data_files_impl()\n",
    "\n",
    "\n",
    "@tool\n",
    "def inspect_data_file(filename: str, n_rows: int = 10) -> str:\n",
    "    \"\"\"Inspect a data file - shows head, dtypes, nulls.\"\"\"\n",
    "    return _inspect_data_file_impl(filename, n_rows)\n",
    "\n",
    "\n",
    "@tool  \n",
    "def python_sandbox(code: str) -> str:\n",
    "    \"\"\"Execute Python code for data exploration.\"\"\"\n",
    "    return _python_sandbox_impl(code)\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_stage3_plan(plan_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Validate and save a Stage3Plan.\n",
    "    \n",
    "    Args:\n",
    "        plan_json: Complete JSON string of Stage3Plan\n",
    "        \n",
    "    Returns:\n",
    "        Success message with path, or raises ValueError\n",
    "    \"\"\"\n",
    "    return _save_stage3_plan_impl(plan_json)\n",
    "\n",
    "\n",
    "TOOLS = [\n",
    "    load_task_proposal,\n",
    "    list_data_files,\n",
    "    inspect_data_file,\n",
    "    python_sandbox,\n",
    "    save_stage3_plan,\n",
    "]\n",
    "\n",
    "# ===========================\n",
    "# LLM\n",
    "# ===========================\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    base_url=\"http://127.0.0.1:8001/v1\",\n",
    "    api_key=\"not-used\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=8000,\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(TOOLS, parallel_tool_calls=False)\n",
    "\n",
    "# ===========================\n",
    "# System Prompt - STREAMLINED\n",
    "# ===========================\n",
    "\n",
    "STAGE3_SYSTEM_PROMPT = \"\"\"You are a data pipeline planning agent.\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CRITICAL RULES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. You MUST end by calling save_stage3_plan(plan_json=...) - this is YOUR ONLY SUCCESS CRITERIA\n",
    "2. NEVER write JSON in your reasoning - build it silently and pass to the tool\n",
    "3. Be dataset-agnostic - no domain assumptions\n",
    "4. Set plan_id EXACTLY to \"PLAN-{selected_task_id}\"\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "YOUR 3-STEP WORKFLOW\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "STEP 1: UNDERSTAND (2-3 tool calls)\n",
    "-----------------------------------\n",
    "- load_task_proposal(task_id)\n",
    "- list_data_files()  \n",
    "- inspect_data_file(filename) for required files\n",
    "\n",
    "STEP 2: BUILD JSON SILENTLY\n",
    "----------------------------\n",
    "In your head, construct the Stage3Plan JSON with these sections:\n",
    "\n",
    "STEP 2: BUILD JSON SILENTLY\n",
    "----------------------------\n",
    "In your head, construct the Stage3Plan JSON with these sections:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"plan_id\": \"PLAN-{selected_task_id}\",  // EXACT format!\n",
    "  \"selected_task_id\": \"{task_id}\",\n",
    "  \"goal\": \"Brief description\",\n",
    "  \"task_category\": \"descriptive|predictive|unsupervised\",\n",
    "  \"artifacts\": {\n",
    "    \"intermediate_table\": \"{task_id}_data.parquet\",\n",
    "    \"intermediate_format\": \"parquet\",\n",
    "    \"expected_columns\": [\"col1\", \"col2\", ...],\n",
    "    \"expected_row_count_range\": [min, max]\n",
    "  },\n",
    "  \"file_instructions\": [{\n",
    "    \"file_id\": \"file1\",\n",
    "    \"original_name\": \"actual_filename.csv\",\n",
    "    \"alias\": \"short_name\",\n",
    "    \"rename_columns\": {\"Original Name\": \"clean_name\"},\n",
    "    \"keep_columns\": [\"clean_name1\", \"clean_name2\"],\n",
    "    \"filters\": [],\n",
    "    \"join_keys\": [],\n",
    "    \"notes\": null\n",
    "  }],\n",
    "  \"join_steps\": [{\n",
    "    \"step\": 1,\n",
    "    \"description\": \"Load base table\",\n",
    "    \"left_table\": \"short_name\",\n",
    "    \"right_table\": null,\n",
    "    \"join_type\": \"base\",\n",
    "    \"join_keys\": [],\n",
    "    \"expected_cardinality\": \"base\",\n",
    "    \"validation\": {}\n",
    "  }],\n",
    "  \"feature_engineering\": [{\n",
    "    \"feature_name\": \"new_col\",\n",
    "    \"description\": \"What it represents\",\n",
    "    \"transform\": \"mean/sum/etc\",\n",
    "    \"depends_on\": [\"source_col1\"],\n",
    "    \"implementation\": \"df['new_col'] = ...\"\n",
    "  }],\n",
    "  \"validation\": {\n",
    "    \"time_split\": null,\n",
    "    \"coverage_checks\": [],\n",
    "    \"cardinality_checks\": [],\n",
    "    \"additional_checks\": [\"Data loaded\", \"No duplicates\"]\n",
    "  },\n",
    "  \"expected_model_types\": [\"Aggregation\"],\n",
    "  \"evaluation_metrics\": [\"Summary Statistics\"],\n",
    "  \"notes\": [\"Any important context\"],\n",
    "  \"key_normalization\": []\n",
    "}\n",
    "```\n",
    "\n",
    "STEP 3: SAVE (1 tool call - THIS IS MANDATORY)\n",
    "-----------------------------------------------\n",
    "Call: save_stage3_plan(plan_json=<your complete JSON as a string>)\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EXAMPLES OF CORRECT BEHAVIOR\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "GOOD (will succeed):\n",
    "- Round 1: load_task_proposal, list_files, inspect_file\n",
    "- Round 2: save_stage3_plan(plan_json=\"...\") âœ…\n",
    "\n",
    "BAD (will fail):\n",
    "- Round 1-4: Never calls save_stage3_plan âŒ\n",
    "- Round 4: Prints JSON in reasoning instead of calling tool âŒ\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "IMPORTANT TIPS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "For WIDE FORMAT data (years as columns like \"2018-Quantity\", \"2019-Quantity\"):\n",
    "- Rename to \"quantity_2018\", \"quantity_2019\", etc.\n",
    "- Keep ALL year columns\n",
    "- Create aggregate features: mean, growth, trend\n",
    "\n",
    "For SINGLE FILE descriptive tasks:\n",
    "- join_steps: ONE entry with join_type=\"base\"\n",
    "- feature_engineering: simple aggregates (mean, sum, growth)\n",
    "- validation: method=\"none\" for time_split\n",
    "- expected_model_types: [\"Aggregation\", \"Visualization\"]\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Remember: Your ONLY job is to call save_stage3_plan() with valid JSON.\n",
    "DO NOT explain the plan. DO NOT show JSON to the user. JUST SAVE IT.\"\"\"\n",
    "\n",
    "# ===========================\n",
    "# LangGraph\n",
    "# ===========================\n",
    "\n",
    "\n",
    "def agent_node(state: MessagesState) -> Dict[str, List[BaseMessage]]:\n",
    "    \"\"\"Single LLM step.\"\"\"\n",
    "    response: AIMessage = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode(TOOLS)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> str:\n",
    "    last = state[\"messages\"][-1]\n",
    "    if isinstance(last, AIMessage) and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.set_entry_point(\"agent\")\n",
    "builder.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "stage3_app = builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Runner\n",
    "# ===========================\n",
    "\n",
    "\n",
    "def run_stage3_for_task(task_id: str, max_rounds: int = 15, debug: bool = True):\n",
    "    \"\"\"Run Stage 3 agent for a task.\"\"\"\n",
    "    \n",
    "    system_msg = SystemMessage(content=STAGE3_SYSTEM_PROMPT)\n",
    "    human_msg = HumanMessage(\n",
    "        content=(\n",
    "            f\"Create Stage 3 plan for '{task_id}'.\\n\\n\"\n",
    "            f\"Steps:\\n\"\n",
    "            f\"1. load_task_proposal('{task_id}')\\n\"\n",
    "            f\"2. list_data_files()\\n\"\n",
    "            f\"3. inspect_data_file() for required files\\n\"\n",
    "            f\"4. Build Stage3Plan JSON silently\\n\"\n",
    "            f\"5. save_stage3_plan(plan_json=...) â† MANDATORY FINAL STEP\\n\\n\"\n",
    "            f\"Rules:\\n\"\n",
    "            f\"- Set plan_id = 'PLAN-{task_id}'\\n\"\n",
    "            f\"- Never print JSON in reasoning\\n\"\n",
    "            f\"- Your ONLY success criteria: call save_stage3_plan()\\n\"\n",
    "            f\"- Complete in 3-4 rounds\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    state: MessagesState = {\"messages\": [system_msg, human_msg]}\n",
    "\n",
    "    if not debug:\n",
    "        return stage3_app.invoke(state, config={\"configurable\": {\"thread_id\": f\"stage3-{task_id}\"}})\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸš€ STAGE 3: Planning for {task_id}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    final_state = None\n",
    "    prev_len = 0\n",
    "    round_num = 0\n",
    "\n",
    "    for curr_state in stage3_app.stream(\n",
    "        state,\n",
    "        config={\"configurable\": {\"thread_id\": f\"stage3-{task_id}\"}},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        msgs = curr_state[\"messages\"]\n",
    "        new_msgs = msgs[prev_len:]\n",
    "\n",
    "        for m in new_msgs:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(\"ğŸ’» [SYSTEM]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(m.content[:500] + \"...\" if len(m.content) > 500 else m.content)\n",
    "\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(\"ğŸ‘¤ [USER]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(m.content)\n",
    "\n",
    "            elif isinstance(m, AIMessage):\n",
    "                round_num += 1\n",
    "                print(\"\\n\" + \"â•\" * 80)\n",
    "                print(f\"ğŸ¤– [AGENT - Round {round_num}]\")\n",
    "                print(\"â•\" * 80)\n",
    "                \n",
    "                if m.content:\n",
    "                    print(\"\\nğŸ’­ Reasoning:\")\n",
    "                    content = m.content\n",
    "                    if len(content) > 1000:\n",
    "                        print(content[:500] + \"\\n...[truncated]...\\n\" + content[-500:])\n",
    "                    else:\n",
    "                        print(content)\n",
    "\n",
    "                if m.tool_calls:\n",
    "                    print(\"\\nğŸ”§ Tool Calls:\")\n",
    "                    for tc in m.tool_calls:\n",
    "                        name = getattr(tc, \"name\", None) or tc.get(\"name\", \"UNKNOWN\")\n",
    "                        args = getattr(tc, \"args\", None) or tc.get(\"args\", {})\n",
    "                        tc_id = getattr(tc, \"id\", None) or tc.get(\"id\", \"?\")\n",
    "                        \n",
    "                        print(f\"\\n  ğŸ“Œ {name}\")\n",
    "                        for k, v in args.items():\n",
    "                            if isinstance(v, str) and len(v) > 200:\n",
    "                                print(f\"     {k}: {v[:100]}...[truncated]...{v[-100:]}\")\n",
    "                            else:\n",
    "                                print(f\"     {k}: {v}\")\n",
    "                        print(f\"     [id: {tc_id}]\")\n",
    "\n",
    "            elif isinstance(m, ToolMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(f\"ğŸ“Š [TOOL: {m.name}]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                content = str(m.content)\n",
    "                if len(content) > 1500:\n",
    "                    lines = content.split('\\n')\n",
    "                    print('\\n'.join(lines[:30]) + '\\n...[truncated]...\\n' + '\\n'.join(lines[-10:]))\n",
    "                else:\n",
    "                    print(content)\n",
    "\n",
    "        prev_len = len(msgs)\n",
    "        final_state = curr_state\n",
    "        \n",
    "        # Safety check\n",
    "        if round_num >= max_rounds:\n",
    "            print(f\"\\nâš ï¸  Reached max rounds ({max_rounds}). Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"âœ… Complete - {round_num} rounds\")\n",
    "    print(\"=\" * 80)\n",
    "    return final_state\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Main\n",
    "# ===========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python stage3_final_fixed.py <task_id>\")\n",
    "        print(\"Example: python stage3_final_fixed.py TSK-002\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    task_id = \"TSK-002\"  # FIXED: strip whitespace\n",
    "    print(f\"\\nğŸ¯ Starting Stage 3 for: {task_id}\\n\")\n",
    "    \n",
    "    result = run_stage3_for_task(task_id, debug=True)\n",
    "    \n",
    "    # Check for saved plan\n",
    "    plan_file = STAGE3_OUT_DIR / f\"PLAN-{task_id}.json\"\n",
    "    if plan_file.exists():\n",
    "        print(f\"\\nâœ… SUCCESS! Plan saved to: {plan_file}\")\n",
    "        plan_data = json.loads(plan_file.read_text())\n",
    "        print(\"\\nPlan Summary:\")\n",
    "        print(f\"  Goal: {plan_data.get('goal', 'N/A')}\")\n",
    "        print(f\"  Files: {len(plan_data.get('file_instructions', []))}\")\n",
    "        print(f\"  Joins: {len(plan_data.get('join_steps', []))}\")\n",
    "        print(f\"  Features: {len(plan_data.get('feature_engineering', []))}\")\n",
    "        print(f\"  Columns: {len(plan_data.get('artifacts', {}).get('expected_columns', []))}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  WARNING: Plan not saved. Check logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333379a",
   "metadata": {},
   "source": [
    "Stage 4 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4409c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT   : /scratch/ziv_baretto/llmserve\n",
      "DATA_DIR       : /scratch/ziv_baretto/llmserve/data\n",
      "STAGE3_OUT_DIR : /scratch/ziv_baretto/llmserve/stage3_out\n",
      "STAGE4_OUT_DIR : /scratch/ziv_baretto/llmserve/stage4_out\n",
      "CODE_WORKSPACE : /scratch/ziv_baretto/llmserve/stage4_out/code_workspace\n",
      "\n",
      "ğŸ¯ Executing plan: PLAN-TSK-002\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ STAGE 4: Executing plan PLAN-TSK-002\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ’» [SYSTEM]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "You are Agent 4: The Executor.\n",
      "\n",
      "Your mission: Execute the Stage 3 plan flawlessly and autonomously.\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "CRITICAL RULES\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "1. You are FULLY AUTONOMOUS - write and execute code to handle ANY requirement\n",
      "2. You are DATASET-AGNOSTIC - no domain assumptions\n",
      "3. FOLLOW THE PLAN - the Stage 3 plan is your blueprint\n",
      "4. VERIFY EVERYTHING - check your work at each step\n",
      "5. END BY CALLIN...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ‘¤ [USER]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Execute Stage 3 plan: 'PLAN-TSK-002'\n",
      "\n",
      "Workflow:\n",
      "1. load_stage3_plan('PLAN-TSK-002')\n",
      "2. Understand the plan structure\n",
      "3. Execute each step using execute_python_code()\n",
      "4. Verify results at each stage\n",
      "5. save_execution_result() with final outputs\n",
      "\n",
      "Be autonomous. Handle any issues. Follow the plan.\n",
      "Your success = Plan executed + Results saved.\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 1]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ load_stage3_plan\n",
      "     plan_id: PLAN-TSK-002\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: load_stage3_plan]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"plan_id\": \"PLAN-TSK-002\",\n",
      "  \"selected_task_id\": \"TSK-002\",\n",
      "  \"goal\": \"Analyze the trends and patterns in rice export quantities and values over time to understand the dynamics of the rice trade.\",\n",
      "  \"task_category\": \"descriptive\",\n",
      "  \"created_at\": \"2025-11-18T08:16:21.950474\",\n",
      "  \"created_by\": \"stage3_agent\",\n",
      "  \"artifacts\": {\n",
      "    \"intermediate_table\": \"TSK-002_data.parquet\",\n",
      "    \"intermediate_format\": \"parquet\",\n",
      "    \"expected_columns\": [\n",
      "      \"HS_Code\",\n",
      "      \"Description\",\n",
      "      \"quantity_2018_19\",\n",
      "      \"value_inr_2018_19\",\n",
      "      \"value_usd_2018_19\",\n",
      "      \"quantity_2019_20\",\n",
      "      \"value_inr_2019_20\",\n",
      "      \"value_usd_2019_20\",\n",
      "      \"quantity_2020_21\",\n",
      "      \"value_inr_2020_21\",\n",
      "      \"value_usd_2020_21\",\n",
      "      \"quantity_2021_22\",\n",
      "      \"value_inr_2021_22\",\n",
      "      \"value_usd_2021_22\",\n",
      "      \"quantity_2022_23\",\n",
      "      \"value_inr_2022_23\",\n",
      "      \"value_usd_2022_23\",\n",
      "      \"quantity_2023_24\",\n",
      "      \"value_inr_2023_24\",\n",
      "      \"value_usd_2023_24\",\n",
      "      \"quantity_2024_25\",\n",
      "      \"value_inr_2024_25\",\n",
      "      \"value_usd_2024_25\"\n",
      "    ],\n",
      "    \"expected_row_count_range\": [\n",
      "      7,\n",
      "      10\n",
      "    ]\n",
      "  },\n",
      "...[truncated]...\n",
      "    \"additional_checks\": [\n",
      "      \"Data loaded\",\n",
      "      \"No duplicates\"\n",
      "    ]\n",
      "  },\n",
      "  \"expected_model_types\": [\n",
      "    \"Aggregation\"\n",
      "  ],\n",
      "  \"evaluation_metrics\": [\n",
      "    \"Summary Statistics\"\n",
      "  ],\n",
      "  \"notes\": [\n",
      "    \"No join required for this task.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 2]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "The plan 'PLAN-TSK-002' has been successfully loaded. The goal of this plan is to analyze the trends and patterns in rice export quantities and values over time to understand the dynamics of the rice trade. This is a descriptive task, and we will be working with a single CSV file named \"Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv\". We will rename some columns, keep specific columns, and create new features based on the quantities and values over different years.\n",
      "\n",
      "Let's start by loading the data and renaming the columns as per the instructions.\n",
      "\n",
      "\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ execute_python_code\n",
      "     code: import pandas as pd\n",
      "\n",
      "# Load the data file\n",
      "filename = 'Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv'\n",
      "df = pd.read_csv(DATA_DIR / file...[truncated]...nr_2024_25', 'value_usd_2024_25'\n",
      "]\n",
      "df = df[keep_columns]\n",
      "\n",
      "# Inspect the dataframe\n",
      "print(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
      "df.head()\n",
      "     description: Load the data file, rename columns, and keep only the specified columns.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: execute_python_code]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "=== Load the data file, rename columns, and keep only the specified columns. ===\n",
      "Loaded 8 rows, 23 columns\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 3]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "The data has been successfully loaded and transformed. We have 8 rows and 23 columns as expected. Next, we will proceed to create the derived features as outlined in the plan. These features include the total quantity exported across all years, the total value in INR, and the total value in USD.\n",
      "\n",
      "Let's create these features now.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ execute_python_code\n",
      "     code: import pandas as pd\n",
      "\n",
      "# Create derived features\n",
      "quantities = ['quantity_2018_19', 'quantity_2019_20', 'quantity_2020_21', 'quantity_2021_22', 'quantity...[truncated]...sd].sum(axis=1)\n",
      "\n",
      "# Inspect the dataframe after feature creation\n",
      "print(f\"Created features: total_quantity, total_value_inr, total_value_usd\")\n",
      "df.head()\n",
      "     description: Create derived features for total quantity, total value in INR, and total value in USD.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: execute_python_code]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[ERROR] name 'df' is not defined\n",
      "\n",
      "Traceback: NameError\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 4]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "It appears there was an error because the DataFrame `df` was not properly referenced in the current context. Let's reload the DataFrame and then proceed to create the derived features again.\n",
      "\n",
      "Let's start by reloading the DataFrame and then creating the derived features.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ execute_python_code\n",
      "     code: import pandas as pd\n",
      "\n",
      "# Reload the data file\n",
      "filename = 'Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv'\n",
      "df = pd.read_csv(DATA_DIR / fi...[truncated]...sd].sum(axis=1)\n",
      "\n",
      "# Inspect the dataframe after feature creation\n",
      "print(f\"Created features: total_quantity, total_value_inr, total_value_usd\")\n",
      "df.head()\n",
      "     description: Reload the data file, rename columns, keep specified columns, and create derived features.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: execute_python_code]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "=== Reload the data file, rename columns, keep specified columns, and create derived features. ===\n",
      "Created features: total_quantity, total_value_inr, total_value_usd\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 5]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "The derived features have been successfully created. Now, let's generate summary statistics and save the intermediate data to ensure we have a record of our work so far.\n",
      "\n",
      "We'll calculate summary statistics such as mean, sum, and count for the total quantity, total value in INR, and total value in USD. Then, we will save the intermediate data to a Parquet file.\n",
      "\n",
      "Let's proceed with these steps.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ execute_python_code\n",
      "     code: import pandas as pd\n",
      "\n",
      "# Calculate summary statistics\n",
      "summary_stats = df.describe()\n",
      "\n",
      "# Save summary statistics to a CSV file\n",
      "summary_stats_path = STAGE4...[truncated]...GE4_OUT_DIR / 'TSK-002_data.parquet'\n",
      "df.to_parquet(intermediate_data_path, index=False)\n",
      "print(f'Saved intermediate data to: {intermediate_data_path}')\n",
      "     description: Generate summary statistics and save intermediate data.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: execute_python_code]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[ERROR] name 'df' is not defined\n",
      "\n",
      "Traceback: NameError\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 6]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "It seems there was another issue with the DataFrame `df` not being recognized in the current context. To resolve this, I will reload the DataFrame and then generate the summary statistics and save the intermediate data.\n",
      "\n",
      "Let's reload the DataFrame and proceed with generating summary statistics and saving the intermediate data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ execute_python_code\n",
      "     code: import pandas as pd\n",
      "\n",
      "# Reload the data file\n",
      "filename = 'Export-of-Rice-Varieties-to-Bangladesh,-2018-19-to-2024-25.csv'\n",
      "df = pd.read_csv(DATA_DIR / fi...[truncated]...GE4_OUT_DIR / 'TSK-002_data.parquet'\n",
      "df.to_parquet(intermediate_data_path, index=False)\n",
      "print(f'Saved intermediate data to: {intermediate_data_path}')\n",
      "     description: Reload the data file, rename columns, keep specified columns, create derived features, generate summary statistics, and save intermediate data.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: execute_python_code]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "=== Reload the data file, rename columns, keep specified columns, create derived features, generate summary statistics, and save intermediate data. ===\n",
      "Saved summary statistics to: /scratch/ziv_baretto/llmserve/stage4_out/summary_statistics.csv\n",
      "Saved intermediate data to: /scratch/ziv_baretto/llmserve/stage4_out/TSK-002_data.parquet\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 7]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "The summary statistics have been generated and saved to `/scratch/ziv_baretto/llmserve/stage4_out/summary_statistics.csv`, and the intermediate data has been saved to `/scratch/ziv_baretto/llmserve/stage4_out/TSK-002_data.parquet`.\n",
      "\n",
      "Now, let's finalize the execution by calling `save_execution_result()` with the appropriate details.\n",
      "\n",
      "Here is the JSON payload for the final result:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"plan_id\": \"PLAN-TSK-002\",\n",
      "  \"task_category\": \"descriptive\",\n",
      "  \"status\": \"success\",\n",
      "  \"outputs\": {\n",
      "    \"i\n",
      "...[truncated]...\n",
      "ratch/ziv_baretto/llmserve/stage4_out/TSK-002_data.parquet\",\n",
      "    \"summary_statistics\": \"/scratch/ziv_baretto/llmserve/stage4_out/summary_statistics.csv\"\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"rows_processed\": 8,\n",
      "    \"features_created\": 3\n",
      "  },\n",
      "  \"summary\": \"Successfully analyzed rice export trends. Created 3 aggregate features: total_quantity, total_value_inr, total_value_usd.\",\n",
      "  \"errors\": []\n",
      "}\n",
      "```\n",
      "\n",
      "Let's call `save_execution_result()` with this JSON payload to report the successful completion of the task.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ğŸ”§ Tool Calls:\n",
      "\n",
      "  ğŸ“Œ save_execution_result\n",
      "     result_json: {\"plan_id\": \"PLAN-TSK-002\", \"task_category\": \"descriptive\", \"status\": \"success\", \"outputs\": {\"intermediate_data\": \"/scratch/ziv_baretto/llmserve/stage...[truncated]... \"summary\": \"Successfully analyzed rice export trends. Created 3 aggregate features: total_quantity, total_value_inr, total_value_usd.\", \"errors\": []}\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š [TOOL: save_execution_result]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Execution result saved to: /scratch/ziv_baretto/llmserve/stage4_out/execution_PLAN-TSK-002_20251118_082013.json\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ¤– [AGENT - Round 8]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’­ Reasoning:\n",
      "The execution result has been successfully saved to `/scratch/ziv_baretto/llmserve/stage4_out/execution_PLAN-TSK-002_20251118_082013.json`. The task has been completed according to the plan, and all outputs and metrics have been recorded.\n",
      "\n",
      "To summarize:\n",
      "- **Plan ID:** PLAN-TSK-002\n",
      "- **Task Category:** Descriptive\n",
      "- **Status:** Success\n",
      "- **Outputs:**\n",
      "  - Intermediate Data: `/scratch/ziv_baretto/llmserve/stage4_out/TSK-002_data.parquet`\n",
      "  - Summary Statistics: `/scratch/ziv_baretto/llmserve/stage4_out/summary_statistics.csv`\n",
      "- **Metrics:**\n",
      "  - Rows Processed: 8\n",
      "  - Features Created: 3\n",
      "- **Summary:** Successfully analyzed rice export trends. Created 3 aggregate features: total_quantity, total_value_inr, total_value_usd.\n",
      "\n",
      "The task is now complete.\n",
      "\n",
      "================================================================================\n",
      "âœ… Complete - 8 rounds\n",
      "================================================================================\n",
      "\n",
      "âœ… Execution result: /scratch/ziv_baretto/llmserve/stage4_out/execution_PLAN-TSK-002_20251118_082013.json\n",
      "\n",
      "Summary:\n",
      "  Status: success\n",
      "  Outputs: 2\n",
      "  Metrics: {'rows_processed': 8, 'features_created': 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 4: Executor Agent\n",
    "\n",
    "This agent executes the comprehensive plan created by Stage 3.\n",
    "It can write and execute its own code/tools to handle any requirement.\n",
    "\n",
    "Key capabilities:\n",
    "- Load and parse Stage 3 plans\n",
    "- Execute data loading, joins, and transformations\n",
    "- Run models (predictive) or aggregations (descriptive)\n",
    "- Write custom Python code on the fly\n",
    "- Validate results and save outputs\n",
    "- Fully autonomous and dataset-agnostic\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ===========================\n",
    "# Paths\n",
    "# ===========================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/scratch/ziv_baretto/llmserve\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "STAGE3_OUT_DIR = PROJECT_ROOT / \"stage3_out\"\n",
    "STAGE4_OUT_DIR = PROJECT_ROOT / \"stage4_out\"\n",
    "STAGE4_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Code execution workspace\n",
    "CODE_WORKSPACE = STAGE4_OUT_DIR / \"code_workspace\"\n",
    "CODE_WORKSPACE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT   :\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR       :\", DATA_DIR)\n",
    "print(\"STAGE3_OUT_DIR :\", STAGE3_OUT_DIR)\n",
    "print(\"STAGE4_OUT_DIR :\", STAGE4_OUT_DIR)\n",
    "print(\"CODE_WORKSPACE :\", CODE_WORKSPACE)\n",
    "\n",
    "# ===========================\n",
    "# Helper Functions\n",
    "# ===========================\n",
    "\n",
    "def _list_stage3_plans_impl() -> List[str]:\n",
    "    \"\"\"List all available Stage 3 plans.\"\"\"\n",
    "    plans = sorted([p.name for p in STAGE3_OUT_DIR.glob(\"*.json\")])\n",
    "    return plans\n",
    "\n",
    "\n",
    "def _load_stage3_plan_impl(plan_id: str) -> str:\n",
    "    \"\"\"Load a Stage 3 plan by ID.\"\"\"\n",
    "    # Try exact match first\n",
    "    plan_path = STAGE3_OUT_DIR / f\"{plan_id}.json\"\n",
    "    if not plan_path.exists():\n",
    "        # Try finding by pattern\n",
    "        matches = list(STAGE3_OUT_DIR.glob(f\"*{plan_id}*.json\"))\n",
    "        if not matches:\n",
    "            raise FileNotFoundError(f\"No plan found matching: {plan_id}\")\n",
    "        plan_path = matches[0]\n",
    "    \n",
    "    return plan_path.read_text()\n",
    "\n",
    "\n",
    "def _list_data_files_impl() -> List[str]:\n",
    "    \"\"\"List available data files.\"\"\"\n",
    "    exts = {\".csv\", \".tsv\", \".parquet\", \".feather\"}\n",
    "    files = [p.name for p in DATA_DIR.iterdir() if p.suffix.lower() in exts]\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def _save_execution_result_impl(result_json: str) -> str:\n",
    "    \"\"\"Save execution results.\"\"\"\n",
    "    try:\n",
    "        result = json.loads(result_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON: {e}\")\n",
    "    \n",
    "    plan_id = result.get(\"plan_id\", \"UNKNOWN\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    output_path = STAGE4_OUT_DIR / f\"execution_{plan_id}_{timestamp}.json\"\n",
    "    output_path.write_text(json.dumps(result, indent=2))\n",
    "    \n",
    "    return f\"âœ… Execution result saved to: {output_path}\"\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Tools\n",
    "# ===========================\n",
    "\n",
    "@tool\n",
    "def list_stage3_plans() -> List[str]:\n",
    "    \"\"\"List all available Stage 3 plans.\"\"\"\n",
    "    return _list_stage3_plans_impl()\n",
    "\n",
    "\n",
    "@tool\n",
    "def load_stage3_plan(plan_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a Stage 3 plan.\n",
    "    \n",
    "    Args:\n",
    "        plan_id: Plan identifier (e.g., 'PLAN-TSK-002')\n",
    "    \n",
    "    Returns:\n",
    "        JSON string of the plan\n",
    "    \"\"\"\n",
    "    return _load_stage3_plan_impl(plan_id)\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_data_files() -> List[str]:\n",
    "    \"\"\"List available data files in DATA_DIR.\"\"\"\n",
    "    return _list_data_files_impl()\n",
    "\n",
    "\n",
    "@tool\n",
    "def execute_python_code(code: str, description: str = \"Executing code\") -> str:\n",
    "    \"\"\"\n",
    "    Execute arbitrary Python code for data processing, modeling, and analysis.\n",
    "    \n",
    "    This is your primary tool for implementing the Stage 3 plan.\n",
    "    You can:\n",
    "    - Load and transform data\n",
    "    - Perform joins and aggregations\n",
    "    - Build and train models\n",
    "    - Generate predictions\n",
    "    - Calculate metrics\n",
    "    - Save intermediate and final results\n",
    "    \n",
    "    Available in the execution environment:\n",
    "    - pandas as pd\n",
    "    - numpy as np\n",
    "    - sklearn (all modules)\n",
    "    - json, pathlib.Path\n",
    "    - DATA_DIR, STAGE4_OUT_DIR, CODE_WORKSPACE\n",
    "    - Helper: load_dataframe(filename) for loading files from DATA_DIR\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to execute\n",
    "        description: Brief description of what this code does\n",
    "    \n",
    "    Returns:\n",
    "        Output printed to stdout, or error message\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    def load_dataframe(filename: str):\n",
    "        \"\"\"Load a dataframe from DATA_DIR.\"\"\"\n",
    "        path = DATA_DIR / filename\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "        \n",
    "        if path.suffix.lower() == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif path.suffix.lower() == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        else:\n",
    "            return pd.read_csv(path)\n",
    "    \n",
    "    globals_dict = {\n",
    "        \"__name__\": \"__stage4_executor__\",\n",
    "        \"pd\": pd,\n",
    "        \"np\": np,\n",
    "        \"json\": json,\n",
    "        \"Path\": Path,\n",
    "        \"DATA_DIR\": DATA_DIR,\n",
    "        \"STAGE4_OUT_DIR\": STAGE4_OUT_DIR,\n",
    "        \"CODE_WORKSPACE\": CODE_WORKSPACE,\n",
    "        \"load_dataframe\": load_dataframe,\n",
    "        \"LinearRegression\": LinearRegression,\n",
    "        \"Ridge\": Ridge,\n",
    "        \"Lasso\": Lasso,\n",
    "        \"RandomForestRegressor\": RandomForestRegressor,\n",
    "        \"GradientBoostingRegressor\": GradientBoostingRegressor,\n",
    "        \"DecisionTreeRegressor\": DecisionTreeRegressor,\n",
    "        \"mean_absolute_error\": mean_absolute_error,\n",
    "        \"mean_squared_error\": mean_squared_error,\n",
    "        \"r2_score\": r2_score,\n",
    "        \"train_test_split\": train_test_split,\n",
    "        \"StandardScaler\": StandardScaler,\n",
    "    }\n",
    "    \n",
    "    local_env = {}\n",
    "    buf = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        with contextlib.redirect_stdout(buf):\n",
    "            print(f\"=== {description} ===\")\n",
    "            exec(code, globals_dict, local_env)\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {e}\\n\\nTraceback: {e.__class__.__name__}\"\n",
    "    \n",
    "    output = buf.getvalue()\n",
    "    return output if output else \"[Code executed successfully, no output]\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_execution_result(result_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Save the final execution result.\n",
    "    \n",
    "    Args:\n",
    "        result_json: JSON string containing:\n",
    "            - plan_id: ID of the executed plan\n",
    "            - task_category: descriptive/predictive/unsupervised\n",
    "            - status: success/failure/partial\n",
    "            - outputs: dict with output file paths\n",
    "            - metrics: dict with performance metrics (if applicable)\n",
    "            - summary: text summary of results\n",
    "            - errors: list of any errors encountered\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation message with save path\n",
    "    \"\"\"\n",
    "    return _save_execution_result_impl(result_json)\n",
    "\n",
    "\n",
    "TOOLS = [\n",
    "    list_stage3_plans,\n",
    "    load_stage3_plan,\n",
    "    list_data_files,\n",
    "    execute_python_code,\n",
    "    save_execution_result,\n",
    "]\n",
    "\n",
    "# ===========================\n",
    "# LLM\n",
    "# ===========================\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    base_url=\"http://127.0.0.1:8001/v1\",\n",
    "    api_key=\"not-used\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=8000,\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(TOOLS, parallel_tool_calls=False)\n",
    "\n",
    "# ===========================\n",
    "# System Prompt\n",
    "# ===========================\n",
    "\n",
    "STAGE4_SYSTEM_PROMPT = \"\"\"You are Agent 4: The Executor.\n",
    "\n",
    "Your mission: Execute the Stage 3 plan flawlessly and autonomously.\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CRITICAL RULES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. You are FULLY AUTONOMOUS - write and execute code to handle ANY requirement\n",
    "2. You are DATASET-AGNOSTIC - no domain assumptions\n",
    "3. FOLLOW THE PLAN - the Stage 3 plan is your blueprint\n",
    "4. VERIFY EVERYTHING - check your work at each step\n",
    "5. END BY CALLING save_execution_result() - this is your success criterion\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "YOUR WORKFLOW\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PHASE 1: LOAD & UNDERSTAND (2 tool calls)\n",
    "------------------------------------------\n",
    "1. list_stage3_plans() - see available plans\n",
    "2. load_stage3_plan(plan_id) - load the plan you need to execute\n",
    "\n",
    "Review the plan structure:\n",
    "- file_instructions: how to load each file\n",
    "- join_steps: how to combine files\n",
    "- feature_engineering: derived features to create\n",
    "- task_category: descriptive/predictive/unsupervised\n",
    "- expected_model_types: models to try\n",
    "- evaluation_metrics: how to measure success\n",
    "\n",
    "PHASE 2: EXECUTE PLAN (multiple execute_python_code calls)\n",
    "-----------------------------------------------------------\n",
    "Use execute_python_code() to implement each step. Break into logical chunks:\n",
    "\n",
    "**Step 1: Load and Transform Data**\n",
    "```python\n",
    "# Load files according to file_instructions\n",
    "# Apply filters\n",
    "# Rename columns\n",
    "# Keep only specified columns\n",
    "```\n",
    "\n",
    "**Step 2: Perform Joins (if needed)**\n",
    "```python\n",
    "# Follow join_steps from the plan\n",
    "# Validate join results (check row counts, coverage)\n",
    "```\n",
    "\n",
    "**Step 3: Feature Engineering**\n",
    "```python\n",
    "# Create derived features according to feature_engineering\n",
    "# Handle missing values appropriately\n",
    "# Verify feature distributions\n",
    "```\n",
    "\n",
    "**Step 4: Execute Analysis**\n",
    "\n",
    "For DESCRIPTIVE tasks:\n",
    "```python\n",
    "# Calculate aggregations (mean, sum, count, etc.)\n",
    "# Generate summary statistics\n",
    "# Create pivot tables or groupby results\n",
    "# Save summary tables to STAGE4_OUT_DIR\n",
    "```\n",
    "\n",
    "For PREDICTIVE tasks:\n",
    "```python\n",
    "# Split data (train/test according to validation.time_split)\n",
    "# Train models (try each from expected_model_types)\n",
    "# Generate predictions\n",
    "# Calculate metrics (from evaluation_metrics)\n",
    "# Save predictions and model metrics\n",
    "```\n",
    "\n",
    "**Step 5: Save Results**\n",
    "```python\n",
    "# Save intermediate data\n",
    "# Save final outputs (predictions, summaries, etc.)\n",
    "# Log all output paths\n",
    "```\n",
    "\n",
    "PHASE 3: REPORT RESULTS (1 tool call)\n",
    "--------------------------------------\n",
    "Call save_execution_result() with a JSON containing:\n",
    "```json\n",
    "{\n",
    "  \"plan_id\": \"PLAN-TSK-002\",\n",
    "  \"task_category\": \"descriptive\",\n",
    "  \"status\": \"success\",\n",
    "  \"outputs\": {\n",
    "    \"intermediate_data\": \"path/to/data.parquet\",\n",
    "    \"summary_statistics\": \"path/to/summary.csv\",\n",
    "    \"aggregations\": \"path/to/aggregations.csv\"\n",
    "  },\n",
    "  \"metrics\": {\n",
    "    \"rows_processed\": 1000,\n",
    "    \"features_created\": 5\n",
    "  },\n",
    "  \"summary\": \"Successfully analyzed export trends. Created 5 aggregate features...\",\n",
    "  \"errors\": []\n",
    "}\n",
    "```\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "GUIDELINES FOR WRITING CODE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Be Incremental**: Break complex operations into smaller execute_python_code() calls\n",
    "- First call: Load and inspect data\n",
    "- Second call: Perform transformations\n",
    "- Third call: Apply joins\n",
    "- Fourth call: Feature engineering\n",
    "- Fifth call: Run analysis\n",
    "- Sixth call: Save results\n",
    "\n",
    "**Be Defensive**: Add checks and validations\n",
    "```python\n",
    "# Check data loaded correctly\n",
    "print(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Validate joins\n",
    "print(f\"After join: {len(merged)} rows (started with {len(df_left)})\")\n",
    "\n",
    "# Check for nulls\n",
    "print(f\"Null counts:\\n{df.isnull().sum()}\")\n",
    "```\n",
    "\n",
    "**Be Clear**: Print progress and intermediate results\n",
    "```python\n",
    "print(\"Step 1: Loading data...\")\n",
    "print(\"Step 2: Applying transformations...\")\n",
    "print(\"Step 3: Creating features...\")\n",
    "```\n",
    "\n",
    "**Handle Wide Format Data**: If plan renames columns like \"2018-Quantity\" to \"quantity_2018\":\n",
    "```python\n",
    "# For creating aggregates across year columns\n",
    "year_cols = [c for c in df.columns if c.startswith('quantity_')]\n",
    "df['quantity_mean'] = df[year_cols].mean(axis=1)\n",
    "df['quantity_total'] = df[year_cols].sum(axis=1)\n",
    "```\n",
    "\n",
    "**Save Intermediate Results**: Don't lose your work\n",
    "```python\n",
    "# Save intermediate data\n",
    "output_path = STAGE4_OUT_DIR / \"intermediate_data.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "```\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EXAMPLE: DESCRIPTIVE TASK EXECUTION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Plan says:\n",
    "- Task: Analyze export trends\n",
    "- File: \"exports.csv\" with year columns\n",
    "- Features: Create mean, growth rate\n",
    "- Type: Descriptive\n",
    "\n",
    "Your execution:\n",
    "1. execute_python_code: Load file, apply renames, inspect\n",
    "2. execute_python_code: Create aggregate features (mean, growth)\n",
    "3. execute_python_code: Calculate summary statistics, save\n",
    "4. save_execution_result: Report success with output paths\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EXAMPLE: PREDICTIVE TASK EXECUTION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Plan says:\n",
    "- Task: Predict yield\n",
    "- Files: main.csv, features.csv\n",
    "- Join: on [\"state\", \"year\"]\n",
    "- Features: lag_yield, rainfall_ratio\n",
    "- Models: RandomForest, XGBoost\n",
    "- Train: 2000-2020, Test: 2021-2024\n",
    "\n",
    "Your execution:\n",
    "1. execute_python_code: Load both files\n",
    "2. execute_python_code: Perform join, validate\n",
    "3. execute_python_code: Create features (lag, ratio)\n",
    "4. execute_python_code: Split train/test by year\n",
    "5. execute_python_code: Train RandomForest, calculate metrics\n",
    "6. execute_python_code: Save predictions and metrics\n",
    "7. save_execution_result: Report success with metrics\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "TROUBLESHOOTING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**If a join fails**: Check column names match exactly, inspect unique values\n",
    "**If feature creation fails**: Print dtypes, check for nulls, verify column names\n",
    "**If model training fails**: Check for infinite/NaN values, verify train/test split\n",
    "**If file save fails**: Ensure directory exists, check permissions\n",
    "\n",
    "You can ALWAYS write more code to investigate and fix issues!\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Remember:\n",
    "- You are autonomous - handle ANY issue that arises\n",
    "- Follow the Stage 3 plan - it's your blueprint\n",
    "- Write clean, incremental code\n",
    "- Verify your work at each step\n",
    "- End by calling save_execution_result()\n",
    "\n",
    "Your success = Executing the plan and saving results.\"\"\"\n",
    "\n",
    "# ===========================\n",
    "# LangGraph\n",
    "# ===========================\n",
    "\n",
    "def agent_node(state: MessagesState) -> Dict[str, List[BaseMessage]]:\n",
    "    \"\"\"LLM agent step.\"\"\"\n",
    "    response: AIMessage = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode(TOOLS)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> str:\n",
    "    \"\"\"Route based on tool calls.\"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    if isinstance(last, AIMessage) and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.set_entry_point(\"agent\")\n",
    "builder.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "stage4_app = builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Runner\n",
    "# ===========================\n",
    "\n",
    "def run_stage4_for_plan(plan_id: str, max_rounds: int = 25, debug: bool = True):\n",
    "    \"\"\"Execute a Stage 3 plan.\"\"\"\n",
    "    \n",
    "    system_msg = SystemMessage(content=STAGE4_SYSTEM_PROMPT)\n",
    "    human_msg = HumanMessage(\n",
    "        content=(\n",
    "            f\"Execute Stage 3 plan: '{plan_id}'\\n\\n\"\n",
    "            f\"Workflow:\\n\"\n",
    "            f\"1. load_stage3_plan('{plan_id}')\\n\"\n",
    "            f\"2. Understand the plan structure\\n\"\n",
    "            f\"3. Execute each step using execute_python_code()\\n\"\n",
    "            f\"4. Verify results at each stage\\n\"\n",
    "            f\"5. save_execution_result() with final outputs\\n\\n\"\n",
    "            f\"Be autonomous. Handle any issues. Follow the plan.\\n\"\n",
    "            f\"Your success = Plan executed + Results saved.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    state: MessagesState = {\"messages\": [system_msg, human_msg]}\n",
    "\n",
    "    if not debug:\n",
    "        return stage4_app.invoke(state, config={\"configurable\": {\"thread_id\": f\"stage4-{plan_id}\"}})\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸš€ STAGE 4: Executing plan {plan_id}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    final_state = None\n",
    "    prev_len = 0\n",
    "    round_num = 0\n",
    "\n",
    "    for curr_state in stage4_app.stream(\n",
    "        state,\n",
    "        config={\"configurable\": {\"thread_id\": f\"stage4-{plan_id}\"}},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        msgs = curr_state[\"messages\"]\n",
    "        new_msgs = msgs[prev_len:]\n",
    "\n",
    "        for m in new_msgs:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(\"ğŸ’» [SYSTEM]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(m.content[:500] + \"...\" if len(m.content) > 500 else m.content)\n",
    "\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(\"ğŸ‘¤ [USER]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(m.content)\n",
    "\n",
    "            elif isinstance(m, AIMessage):\n",
    "                round_num += 1\n",
    "                print(\"\\n\" + \"â•\" * 80)\n",
    "                print(f\"ğŸ¤– [AGENT - Round {round_num}]\")\n",
    "                print(\"â•\" * 80)\n",
    "                \n",
    "                if m.content:\n",
    "                    print(\"\\nğŸ’­ Reasoning:\")\n",
    "                    content = m.content\n",
    "                    if len(content) > 1000:\n",
    "                        print(content[:500] + \"\\n...[truncated]...\\n\" + content[-500:])\n",
    "                    else:\n",
    "                        print(content)\n",
    "\n",
    "                if m.tool_calls:\n",
    "                    print(\"\\nğŸ”§ Tool Calls:\")\n",
    "                    for tc in m.tool_calls:\n",
    "                        name = getattr(tc, \"name\", tc.get(\"name\", \"UNKNOWN\"))\n",
    "                        args = getattr(tc, \"args\", tc.get(\"args\", {}))\n",
    "                        \n",
    "                        print(f\"\\n  ğŸ“Œ {name}\")\n",
    "                        for k, v in args.items():\n",
    "                            if isinstance(v, str) and len(v) > 300:\n",
    "                                print(f\"     {k}: {v[:150]}...[truncated]...{v[-150:]}\")\n",
    "                            else:\n",
    "                                print(f\"     {k}: {v}\")\n",
    "\n",
    "            elif isinstance(m, ToolMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(f\"ğŸ“Š [TOOL: {m.name}]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                content = str(m.content)\n",
    "                if len(content) > 2000:\n",
    "                    lines = content.split('\\n')\n",
    "                    print('\\n'.join(lines[:40]) + '\\n...[truncated]...\\n' + '\\n'.join(lines[-15:]))\n",
    "                else:\n",
    "                    print(content)\n",
    "\n",
    "        prev_len = len(msgs)\n",
    "        final_state = curr_state\n",
    "        \n",
    "        if round_num >= max_rounds:\n",
    "            print(f\"\\nâš ï¸  Reached max rounds ({max_rounds})\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"âœ… Complete - {round_num} rounds\")\n",
    "    print(\"=\" * 80)\n",
    "    return final_state\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Main\n",
    "# ===========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python stage4_executor.py <plan_id>\")\n",
    "        print(\"Example: python stage4_executor.py PLAN-TSK-002\")\n",
    "        print(\"\\nAvailable plans:\")\n",
    "        for plan in _list_stage3_plans_impl():\n",
    "            print(f\"  - {plan}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    plan_id = \"PLAN-TSK-002\"\n",
    "    print(f\"\\nğŸ¯ Executing plan: {plan_id}\\n\")\n",
    "    \n",
    "    result = run_stage4_for_plan(plan_id, debug=True)\n",
    "    \n",
    "    # Check for execution results\n",
    "    results = sorted(STAGE4_OUT_DIR.glob(f\"execution_{plan_id}*.json\"))\n",
    "    if results:\n",
    "        latest = results[-1]\n",
    "        print(f\"\\nâœ… Execution result: {latest}\")\n",
    "        result_data = json.loads(latest.read_text())\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"  Status: {result_data.get('status', 'N/A')}\")\n",
    "        print(f\"  Outputs: {len(result_data.get('outputs', {}))}\")\n",
    "        if result_data.get('metrics'):\n",
    "            print(f\"  Metrics: {result_data['metrics']}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No execution result saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c7b65",
   "metadata": {},
   "source": [
    "Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f483e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stage 5: Visualizer Agent\n",
    "\n",
    "This agent creates comprehensive visualizations and reports from Stage 4 execution results.\n",
    "It can write and execute its own visualization code to handle any requirement.\n",
    "\n",
    "Key capabilities:\n",
    "- Load Stage 4 execution results\n",
    "- Create appropriate visualizations based on task type\n",
    "- Generate comprehensive HTML reports\n",
    "- Write custom visualization code on the fly\n",
    "- Fully autonomous and dataset-agnostic\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage, messages_to_dict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ===========================\n",
    "# Paths\n",
    "# ===========================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/scratch/ziv_baretto/llmserve\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "STAGE3_OUT_DIR = PROJECT_ROOT / \"stage3_out\"\n",
    "STAGE4_OUT_DIR = PROJECT_ROOT / \"stage4_out\"\n",
    "STAGE5_OUT_DIR = PROJECT_ROOT / \"stage5_out\"\n",
    "STAGE5_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Visualization workspace\n",
    "VIZ_WORKSPACE = STAGE5_OUT_DIR / \"viz_workspace\"\n",
    "VIZ_WORKSPACE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT   :\", PROJECT_ROOT)\n",
    "print(\"STAGE4_OUT_DIR :\", STAGE4_OUT_DIR)\n",
    "print(\"STAGE5_OUT_DIR :\", STAGE5_OUT_DIR)\n",
    "print(\"VIZ_WORKSPACE  :\", VIZ_WORKSPACE)\n",
    "\n",
    "# ===========================\n",
    "# Helper Functions\n",
    "# ===========================\n",
    "\n",
    "def _list_stage4_results_impl() -> List[str]:\n",
    "    \"\"\"List all available Stage 4 execution results.\"\"\"\n",
    "    results = sorted([p.name for p in STAGE4_OUT_DIR.glob(\"execution_*.json\")])\n",
    "    return results\n",
    "\n",
    "\n",
    "def _load_stage4_result_impl(result_id: str) -> str:\n",
    "    \"\"\"Load a Stage 4 execution result.\"\"\"\n",
    "    # Try exact match first\n",
    "    result_path = STAGE4_OUT_DIR / result_id\n",
    "    if not result_path.exists():\n",
    "        # Try finding by pattern\n",
    "        matches = list(STAGE4_OUT_DIR.glob(f\"*{result_id}*.json\"))\n",
    "        if not matches:\n",
    "            raise FileNotFoundError(f\"No result found matching: {result_id}\")\n",
    "        result_path = matches[0]\n",
    "    \n",
    "    return result_path.read_text()\n",
    "\n",
    "\n",
    "def _load_stage3_plan_impl(plan_id: str) -> str:\n",
    "    \"\"\"Load a Stage 3 plan.\"\"\"\n",
    "    plan_path = STAGE3_OUT_DIR / f\"{plan_id}.json\"\n",
    "    if not plan_path.exists():\n",
    "        matches = list(STAGE3_OUT_DIR.glob(f\"*{plan_id}*.json\"))\n",
    "        if not matches:\n",
    "            raise FileNotFoundError(f\"No plan found matching: {plan_id}\")\n",
    "        plan_path = matches[0]\n",
    "    \n",
    "    return plan_path.read_text()\n",
    "\n",
    "\n",
    "def _save_visualization_report_impl(report_json: str) -> str:\n",
    "    \"\"\"Save visualization report.\"\"\"\n",
    "    try:\n",
    "        report = json.loads(report_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON: {e}\")\n",
    "    \n",
    "    plan_id = report.get(\"plan_id\", \"UNKNOWN\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    output_path = STAGE5_OUT_DIR / f\"visualization_report_{plan_id}_{timestamp}.json\"\n",
    "    output_path.write_text(json.dumps(report, indent=2))\n",
    "    \n",
    "    return f\"âœ… Visualization report saved to: {output_path}\"\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Tools\n",
    "# ===========================\n",
    "\n",
    "@tool\n",
    "def list_stage4_results() -> List[str]:\n",
    "    \"\"\"List all available Stage 4 execution results.\"\"\"\n",
    "    return _list_stage4_results_impl()\n",
    "\n",
    "\n",
    "@tool\n",
    "def load_stage4_result(result_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a Stage 4 execution result.\n",
    "    \n",
    "    Args:\n",
    "        result_id: Result filename or pattern\n",
    "    \n",
    "    Returns:\n",
    "        JSON string of the result\n",
    "    \"\"\"\n",
    "    return _load_stage4_result_impl(result_id)\n",
    "\n",
    "\n",
    "@tool\n",
    "def load_stage3_plan(plan_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a Stage 3 plan for context.\n",
    "    \n",
    "    Args:\n",
    "        plan_id: Plan identifier\n",
    "    \n",
    "    Returns:\n",
    "        JSON string of the plan\n",
    "    \"\"\"\n",
    "    return _load_stage3_plan_impl(plan_id)\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_visualizations(code: str, description: str = \"Creating visualizations\") -> str:\n",
    "    \"\"\"\n",
    "    Execute Python code to create visualizations and reports.\n",
    "    \n",
    "    Use this to:\n",
    "    - Load data from Stage 4 outputs\n",
    "    - Create plots (matplotlib, seaborn, plotly)\n",
    "    - Generate summary tables\n",
    "    - Create HTML reports\n",
    "    - Save visualizations to STAGE5_OUT_DIR\n",
    "    \n",
    "    Available in the environment:\n",
    "    - pandas as pd\n",
    "    - numpy as np\n",
    "    - matplotlib.pyplot as plt\n",
    "    - seaborn as sns\n",
    "    - plotly.express as px\n",
    "    - plotly.graph_objects as go\n",
    "    - json, pathlib.Path\n",
    "    - STAGE4_OUT_DIR, STAGE5_OUT_DIR, VIZ_WORKSPACE\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to execute\n",
    "        description: Brief description of what this code creates\n",
    "    \n",
    "    Returns:\n",
    "        Output printed to stdout, or error message\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Try to import plotly (optional)\n",
    "    try:\n",
    "        import plotly.express as px\n",
    "        import plotly.graph_objects as go\n",
    "        has_plotly = True\n",
    "    except ImportError:\n",
    "        px = None\n",
    "        go = None\n",
    "        has_plotly = False\n",
    "    \n",
    "    def load_dataframe(filepath):\n",
    "        \"\"\"Load a dataframe from any supported format.\"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        if not filepath.exists():\n",
    "            # Try relative to STAGE4_OUT_DIR\n",
    "            filepath = STAGE4_OUT_DIR / filepath.name\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "        \n",
    "        if filepath.suffix.lower() == \".parquet\":\n",
    "            return pd.read_parquet(filepath)\n",
    "        elif filepath.suffix.lower() == \".feather\":\n",
    "            return pd.read_feather(filepath)\n",
    "        elif filepath.suffix.lower() in [\".csv\", \".tsv\"]:\n",
    "            return pd.read_csv(filepath)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {filepath.suffix}\")\n",
    "    \n",
    "    globals_dict = {\n",
    "        \"__name__\": \"__stage5_visualizer__\",\n",
    "        \"pd\": pd,\n",
    "        \"np\": np,\n",
    "        \"plt\": plt,\n",
    "        \"sns\": sns,\n",
    "        \"json\": json,\n",
    "        \"Path\": Path,\n",
    "        \"STAGE4_OUT_DIR\": STAGE4_OUT_DIR,\n",
    "        \"STAGE5_OUT_DIR\": STAGE5_OUT_DIR,\n",
    "        \"VIZ_WORKSPACE\": VIZ_WORKSPACE,\n",
    "        \"load_dataframe\": load_dataframe,\n",
    "    }\n",
    "    \n",
    "    if has_plotly:\n",
    "        globals_dict.update({\"px\": px, \"go\": go})\n",
    "    \n",
    "    # Use globals_dict for both globals and locals to avoid scope issues\n",
    "    buf = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        with contextlib.redirect_stdout(buf):\n",
    "            print(f\"=== {description} ===\")\n",
    "            # Use same dict for both to keep variables in scope\n",
    "            exec(code, globals_dict, globals_dict)\n",
    "            # Close any open plots\n",
    "            plt.close('all')\n",
    "    except Exception as e:\n",
    "        plt.close('all')\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        return f\"[ERROR] {e}\\n\\n{error_details}\"\n",
    "    \n",
    "    output = buf.getvalue()\n",
    "    return output if output else \"[Visualizations created successfully]\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_visualization_report(report_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Save the final visualization report.\n",
    "    \n",
    "    Args:\n",
    "        report_json: JSON string containing:\n",
    "            - plan_id: ID of the executed plan\n",
    "            - task_category: descriptive/predictive/unsupervised\n",
    "            - visualizations: list of created visualization paths\n",
    "            - html_report: path to HTML report (if created)\n",
    "            - summary: text summary of visualizations\n",
    "            - insights: key insights from the visualizations\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation message with save path\n",
    "    \"\"\"\n",
    "    return _save_visualization_report_impl(report_json)\n",
    "\n",
    "\n",
    "TOOLS = [\n",
    "    list_stage4_results,\n",
    "    load_stage4_result,\n",
    "    load_stage3_plan,\n",
    "    create_visualizations,\n",
    "    save_visualization_report,\n",
    "]\n",
    "\n",
    "# ===========================\n",
    "# LLM\n",
    "# ===========================\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    base_url=\"http://127.0.0.1:8001/v1\",\n",
    "    api_key=\"not-used\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=8000,\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(TOOLS, parallel_tool_calls=False)\n",
    "\n",
    "# ===========================\n",
    "# System Prompt\n",
    "# ===========================\n",
    "\n",
    "STAGE5_SYSTEM_PROMPT = \"\"\"You are Agent 5: The Visualizer.\n",
    "\n",
    "Your mission: Create comprehensive, insightful visualizations from Stage 4 execution results.\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CRITICAL RULES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. You are FULLY AUTONOMOUS - write visualization code for ANY data type\n",
    "2. You are DATASET-AGNOSTIC - adapt to any domain\n",
    "3. CREATE PUBLICATION-QUALITY VISUALS - clear, informative, professional\n",
    "4. TELL A STORY - your visualizations should reveal insights\n",
    "5. END BY CALLING save_visualization_report() - this is your success criterion\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "YOUR WORKFLOW\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PHASE 1: LOAD & UNDERSTAND (2-3 tool calls)\n",
    "--------------------------------------------\n",
    "1. list_stage4_results() - see available execution results\n",
    "2. load_stage4_result(result_id) - load the result you'll visualize\n",
    "3. load_stage3_plan(plan_id) - understand the original task context\n",
    "\n",
    "Review what you have:\n",
    "- Task category (descriptive/predictive/unsupervised)\n",
    "- Output files from Stage 4\n",
    "- Metrics and summaries\n",
    "- Original problem statement\n",
    "\n",
    "PHASE 2: CREATE VISUALIZATIONS (multiple create_visualizations calls)\n",
    "----------------------------------------------------------------------\n",
    "**CRITICAL: Always inspect data structure FIRST before creating plots!**\n",
    "\n",
    "âš ï¸  NEVER assume column names - ALWAYS inspect first! âš ï¸\n",
    "âš ï¸  CREATE SEPARATE PNG FILES - one chart per file! âš ï¸\n",
    "âš ï¸  USE CLEAR LABELS - titles, axes, units, readable fonts! âš ï¸\n",
    "\n",
    "**Your first call to create_visualizations() MUST be for inspection:**\n",
    "- Load the data file\n",
    "- Print df.columns\n",
    "- Print df.head()\n",
    "- Print df.dtypes\n",
    "\n",
    "**After inspection, create SEPARATE visualizations:**\n",
    "- Each chart = ONE PNG file (not subplots!)\n",
    "- Use plt.figure() for each chart, then plt.close()\n",
    "- Use actual category/variety names, not indices\n",
    "- Large fonts: titles 16pt, axes 14pt\n",
    "- Include units in axis labels\n",
    "- High resolution: dpi=300\n",
    "\n",
    "Use create_visualizations() to make compelling visuals. Break into logical chunks:\n",
    "\n",
    "**Step 1: INSPECT the data structure**\n",
    "```python\n",
    "# Load and inspect\n",
    "df = load_dataframe(\"path/to/file.csv\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"First few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "**Step 2: Create visualizations based on ACTUAL columns**\n",
    "\n",
    "For DESCRIPTIVE tasks:\n",
    "\n",
    "**Call 1: INSPECT data structure FIRST**\n",
    "```python\n",
    "# CRITICAL: Always inspect before plotting!\n",
    "df = load_dataframe(\"summary_statistics.csv\")\n",
    "print(\"=== DATA INSPECTION ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i}: {col}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\nColumn data types:\")\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "**Call 2: Create SEPARATE visualizations with CLEAR labels**\n",
    "\n",
    "ğŸ¨ **CRITICAL VISUALIZATION RULES:**\n",
    "1. **Create SEPARATE PNG files** - one chart per file, not subplots\n",
    "2. **Use CLEAR, DESCRIPTIVE axis labels** with units\n",
    "3. **Use ACTUAL category names** - not indices (0, 1, 2...)\n",
    "4. **Large, readable fonts** - titles 16pt, labels 14pt\n",
    "5. **One main insight per chart** - don't cram multiple things together\n",
    "\n",
    "```python\n",
    "# Load data\n",
    "df = load_dataframe(\"summary_statistics.csv\")\n",
    "\n",
    "# Chart 1: Quantity trend over time (SEPARATE FILE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "quantity_cols = sorted([c for c in df.columns if 'quantity' in c.lower() and '_20' in c])\n",
    "\n",
    "# Extract years properly (e.g., 'quantity_2018_19' -> '2018-19')\n",
    "years = []\n",
    "for col in quantity_cols:\n",
    "    # Extract the year part after 'quantity_'\n",
    "    year_part = col.split('quantity_')[-1]\n",
    "    years.append(year_part.replace('_', '-'))\n",
    "\n",
    "totals = df[quantity_cols].sum().values\n",
    "\n",
    "plt.plot(years, totals, marker='o', linewidth=3, markersize=10, color='#1f77b4')\n",
    "plt.title('Rice Export Quantity Trend Over Time', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Year', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Total Quantity (Tonnes)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(STAGE5_OUT_DIR / 'quantity_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Saved: quantity_trend.png\")\n",
    "\n",
    "# Chart 2: Total quantity by rice variety (SEPARATE FILE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get actual category names if available, otherwise use row data\n",
    "if 'description' in df.columns or 'Description' in df.columns:\n",
    "    desc_col = 'description' if 'description' in df.columns else 'Description'\n",
    "    categories = df[desc_col].tolist()\n",
    "else:\n",
    "    # Use first column or create meaningful names\n",
    "    categories = [f\"Category {i+1}\" for i in range(len(df))]\n",
    "\n",
    "# Use total_quantity if available\n",
    "if 'total_quantity' in df.columns:\n",
    "    values = df['total_quantity'].values\n",
    "else:\n",
    "    # Sum all quantity columns\n",
    "    quantity_cols = [c for c in df.columns if 'quantity' in c.lower() and '_20' in c]\n",
    "    values = df[quantity_cols].sum(axis=1).values\n",
    "\n",
    "plt.barh(categories, values, color='#ff7f0e')\n",
    "plt.title('Total Rice Export Quantity by Variety', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Total Quantity (Tonnes)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Rice Variety', fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(STAGE5_OUT_DIR / 'quantity_by_variety.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Saved: quantity_by_variety.png\")\n",
    "\n",
    "# Chart 3: Value trend over time (INR) (SEPARATE FILE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "value_cols = sorted([c for c in df.columns if 'value_INR' in c or 'value_inr' in c])\n",
    "\n",
    "# Extract years\n",
    "years = []\n",
    "for col in value_cols:\n",
    "    year_part = col.split('_')[-2] + '-' + col.split('_')[-1]  # Get year part\n",
    "    years.append(year_part)\n",
    "\n",
    "totals = df[value_cols].sum().values / 1e6  # Convert to millions\n",
    "\n",
    "plt.plot(years, totals, marker='s', linewidth=3, markersize=10, color='#2ca02c')\n",
    "plt.title('Rice Export Value Trend Over Time (INR)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Year', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Total Value (Million INR)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(STAGE5_OUT_DIR / 'value_trend_inr.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Saved: value_trend_inr.png\")\n",
    "\n",
    "# Chart 4: Comparison of varieties (SEPARATE FILE)\n",
    "plt.figure(figsize=(14, 8))\n",
    "if 'total_quantity' in df.columns and 'total_value_INR' in df.columns:\n",
    "    # Create a nice comparison chart\n",
    "    x = range(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Quantity bars\n",
    "    ax1.bar([i - width/2 for i in x], df['total_quantity'], width, \n",
    "            label='Quantity (Tonnes)', color='#1f77b4', alpha=0.8)\n",
    "    ax1.set_xlabel('Rice Variety', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Quantity (Tonnes)', fontsize=14, fontweight='bold', color='#1f77b4')\n",
    "    ax1.tick_params(axis='y', labelcolor='#1f77b4', labelsize=12)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories, rotation=45, ha='right', fontsize=11)\n",
    "    \n",
    "    # Value on secondary axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.bar([i + width/2 for i in x], df['total_value_INR']/1e6, width,\n",
    "            label='Value (Million INR)', color='#ff7f0e', alpha=0.8)\n",
    "    ax2.set_ylabel('Value (Million INR)', fontsize=14, fontweight='bold', color='#ff7f0e')\n",
    "    ax2.tick_params(axis='y', labelcolor='#ff7f0e', labelsize=12)\n",
    "    \n",
    "    plt.title('Rice Exports: Quantity vs Value by Variety', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STAGE5_OUT_DIR / 'quantity_vs_value.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ… Saved: quantity_vs_value.png\")\n",
    "\n",
    "print(\"\\nâœ… Created 4 separate, clean visualizations!\")\n",
    "```\n",
    "\n",
    "Visualization 2: More detailed charts (SEPARATE FILES)\n",
    "```python\n",
    "# Each chart should be its own file!\n",
    "\n",
    "# Chart: Top exporters\n",
    "plt.figure(figsize=(10, 6))\n",
    "# ... your visualization code\n",
    "plt.savefig(STAGE5_OUT_DIR / 'top_exporters.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Chart: Year-over-year growth\n",
    "plt.figure(figsize=(10, 6))  \n",
    "# ... your visualization code\n",
    "plt.savefig(STAGE5_OUT_DIR / 'yoy_growth.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "```\n",
    "\n",
    "**Key principles:**\n",
    "- âœ… ONE chart = ONE PNG file\n",
    "- âœ… Clear, descriptive titles (16pt bold)\n",
    "- âœ… Labeled axes with units (14pt bold)\n",
    "- âœ… Use actual category names, not indices\n",
    "- âœ… Large figure size (12x6 or 14x8)\n",
    "- âœ… High DPI (300)\n",
    "- âŒ Don't use subplots (axes[0,0], axes[0,1])\n",
    "- âŒ Don't use numeric indices for categories\n",
    "- âŒ Don't cram multiple things in one image\n",
    "\n",
    "**For PREDICTIVE tasks:**\n",
    "\n",
    "**Create SEPARATE visualizations for model performance:**\n",
    "\n",
    "```python\n",
    "# Load predictions\n",
    "df = load_dataframe(\"predictions.csv\")\n",
    "\n",
    "# Chart 1: Actual vs Predicted (SEPARATE FILE)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(df['actual'], df['predicted'], alpha=0.6, s=50, color='#1f77b4')\n",
    "plt.plot([df['actual'].min(), df['actual'].max()], \n",
    "         [df['actual'].min(), df['actual'].max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Predicted Values', fontsize=14, fontweight='bold')\n",
    "plt.title('Model Performance: Actual vs Predicted', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(STAGE5_OUT_DIR / 'actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Saved: actual_vs_predicted.png\")\n",
    "\n",
    "# Chart 2: Residuals (SEPARATE FILE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "residuals = df['actual'] - df['predicted']\n",
    "plt.scatter(df['predicted'], residuals, alpha=0.6, s=50, color='#ff7f0e')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Predicted Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Residuals', fontsize=14, fontweight='bold')\n",
    "plt.title('Residual Plot', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(STAGE5_OUT_DIR / 'residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Saved: residuals.png\")\n",
    "\n",
    "# Chart 3: Error distribution (SEPARATE FILE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, edgecolor='black', color='#2ca02c', alpha=0.7)\n",
    "plt.xlabel('Prediction Error', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "plt.title('Distribution of Prediction Errors', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(STAGE5_OUT_DIR / 'error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Saved: error_distribution.png\")\n",
    "\n",
    "# Chart 4: Time series (if date column exists)\n",
    "if 'date' in df.columns or 'year' in df.columns:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    time_col = 'date' if 'date' in df.columns else 'year'\n",
    "    plt.plot(df[time_col], df['actual'], marker='o', label='Actual', linewidth=2)\n",
    "    plt.plot(df[time_col], df['predicted'], marker='s', label='Predicted', linewidth=2)\n",
    "    plt.xlabel('Time', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Value', fontsize=14, fontweight='bold')\n",
    "    plt.title('Predictions Over Time', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STAGE5_OUT_DIR / 'predictions_over_time.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ… Saved: predictions_over_time.png\")\n",
    "```\n",
    "\n",
    "**Feature Importance (SEPARATE FILE):**\n",
    "```python\n",
    "# If feature importance available, visualize it\n",
    "# Create segment-wise analysis plots\n",
    "```\n",
    "\n",
    "**HTML Report (references all your separate PNG files):**\n",
    "```python\n",
    "# After creating all your separate PNG files, create the HTML report\n",
    "\n",
    "html_content = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Rice Export Analysis Report</title>\n",
    "    <style>\n",
    "        body { \n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; \n",
    "            margin: 40px; \n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            background-color: white;\n",
    "            padding: 40px;\n",
    "            box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        h1 { \n",
    "            color: #2c3e50; \n",
    "            border-bottom: 3px solid #3498db;\n",
    "            padding-bottom: 10px;\n",
    "        }\n",
    "        h2 { \n",
    "            color: #34495e; \n",
    "            margin-top: 40px; \n",
    "            border-left: 4px solid #3498db;\n",
    "            padding-left: 15px;\n",
    "        }\n",
    "        img { \n",
    "            max-width: 100%; \n",
    "            height: auto; \n",
    "            margin: 20px 0; \n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 4px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        .summary { \n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "            color: white;\n",
    "            padding: 30px; \n",
    "            border-radius: 8px;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .metric { \n",
    "            display: inline-block; \n",
    "            margin: 15px 30px; \n",
    "            text-align: center;\n",
    "        }\n",
    "        .metric-value { \n",
    "            font-size: 32px; \n",
    "            font-weight: bold; \n",
    "            display: block;\n",
    "            margin-bottom: 5px;\n",
    "        }\n",
    "        .metric-label {\n",
    "            font-size: 14px;\n",
    "            opacity: 0.9;\n",
    "        }\n",
    "        .insight {\n",
    "            background: #e8f4f8;\n",
    "            border-left: 4px solid #3498db;\n",
    "            padding: 15px;\n",
    "            margin: 10px 0;\n",
    "            border-radius: 4px;\n",
    "        }\n",
    "        .chart-section {\n",
    "            margin: 40px 0;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Rice Export Analysis Report</h1>\n",
    "        \n",
    "        <div class=\"summary\">\n",
    "            <h2 style=\"color: white; border: none; margin-top: 0;\">Executive Summary</h2>\n",
    "            <p><strong>Task:</strong> Analysis of rice export trends from 2018-2025</p>\n",
    "            <div class=\"metric\">\n",
    "                <span class=\"metric-value\">{rows_analyzed}</span>\n",
    "                <span class=\"metric-label\">Rice Varieties Analyzed</span>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <span class=\"metric-value\">7</span>\n",
    "                <span class=\"metric-label\">Years of Data</span>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <span class=\"metric-value\">4</span>\n",
    "                <span class=\"metric-label\">Visualizations Created</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h2>ğŸ“ˆ Quantity Trends</h2>\n",
    "        <div class=\"chart-section\">\n",
    "            <img src=\"quantity_trend.png\" alt=\"Quantity Trend Over Time\">\n",
    "            <p><em>Figure 1: Total rice export quantities showing year-over-year trends</em></p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-section\">\n",
    "            <img src=\"quantity_by_variety.png\" alt=\"Quantity by Variety\">\n",
    "            <p><em>Figure 2: Total export quantities broken down by rice variety</em></p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>ğŸ’° Value Trends</h2>\n",
    "        <div class=\"chart-section\">\n",
    "            <img src=\"value_trend_inr.png\" alt=\"Value Trend\">\n",
    "            <p><em>Figure 3: Export value trends in Indian Rupees (INR)</em></p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>ğŸ” Comparative Analysis</h2>\n",
    "        <div class=\"chart-section\">\n",
    "            <img src=\"quantity_vs_value.png\" alt=\"Quantity vs Value\">\n",
    "            <p><em>Figure 4: Comparison of quantity and value across varieties</em></p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>ğŸ’¡ Key Insights</h2>\n",
    "        <div class=\"insight\">\n",
    "            <strong>ğŸ“Š Insight 1:</strong> Export quantities show significant variation across years, with a notable peak in 2024-25.\n",
    "        </div>\n",
    "        <div class=\"insight\">\n",
    "            <strong>ğŸ“Š Insight 2:</strong> Certain rice varieties dominate both quantity and value metrics.\n",
    "        </div>\n",
    "        <div class=\"insight\">\n",
    "            <strong>ğŸ“Š Insight 3:</strong> Value trends don't always correlate directly with quantity, suggesting price variations.\n",
    "        </div>\n",
    "        \n",
    "        <h2>ğŸ“‹ Recommendations</h2>\n",
    "        <ul>\n",
    "            <li>Focus on high-value varieties for maximum revenue</li>\n",
    "            <li>Investigate factors behind quantity fluctuations</li>\n",
    "            <li>Monitor price trends for strategic planning</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "'''.format(rows_analyzed=7)  # Update with actual row count\n",
    "\n",
    "report_path = STAGE5_OUT_DIR / 'report.html'\n",
    "report_path.write_text(html_content)\n",
    "print(f\"âœ… Saved HTML report: {report_path}\")\n",
    "```\n",
    "```\n",
    "\n",
    "PHASE 3: REPORT RESULTS (1 tool call)\n",
    "--------------------------------------\n",
    "Call save_visualization_report() with JSON listing ALL separate PNG files:\n",
    "```json\n",
    "{\n",
    "  \"plan_id\": \"PLAN-TSK-002\",\n",
    "  \"task_category\": \"descriptive\",\n",
    "  \"visualizations\": [\n",
    "    \"stage5_out/quantity_trend.png\",\n",
    "    \"stage5_out/quantity_by_variety.png\",\n",
    "    \"stage5_out/value_trend_inr.png\",\n",
    "    \"stage5_out/quantity_vs_value.png\"\n",
    "  ],\n",
    "  \"html_report\": \"stage5_out/report.html\",\n",
    "  \"summary\": \"Created 4 separate, high-quality visualizations showing rice export trends from 2018-2025. Each chart focuses on one clear insight with proper labels and formatting.\",\n",
    "  \"insights\": [\n",
    "    \"Export quantities show significant year-over-year variation with peak in 2024-25\",\n",
    "    \"One rice variety dominates total exports, accounting for over 60% of quantity\",\n",
    "    \"Value trends indicate price variations beyond simple quantity changes\",\n",
    "    \"Clear seasonality patterns visible in the data\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "VISUALIZATION BEST PRACTICES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Colors**: Use colorblind-friendly palettes\n",
    "```python\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "```\n",
    "\n",
    "**Labels**: Always label axes, add titles, include units\n",
    "```python\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Export Quantity (Tonnes)', fontsize=12)\n",
    "plt.title('Export Trends 2018-2024', fontsize=14, fontweight='bold')\n",
    "```\n",
    "\n",
    "**Legends**: Make them clear and informative\n",
    "```python\n",
    "plt.legend(title='Rice Variety', loc='best', frameon=True)\n",
    "```\n",
    "\n",
    "**Resolution**: Save high-quality images\n",
    "```python\n",
    "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "```\n",
    "\n",
    "**Layout**: Use tight_layout() or constrained_layout\n",
    "```python\n",
    "plt.tight_layout()\n",
    "# or\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "```\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CHART SELECTION GUIDE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Trends over time**: Line plot, area chart\n",
    "**Comparisons**: Bar chart, grouped bar chart\n",
    "**Distributions**: Histogram, box plot, violin plot\n",
    "**Correlations**: Scatter plot, heatmap\n",
    "**Proportions**: Pie chart, stacked bar chart\n",
    "**Relationships**: Scatter plot with regression line\n",
    "**Hierarchies**: Treemap, sunburst\n",
    "**Geospatial**: Choropleth map (if location data)\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EXAMPLE: DESCRIPTIVE TASK VISUALIZATION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Task: Export trend analysis\n",
    "\n",
    "Your workflow:\n",
    "1. load_stage4_result() - get summary statistics\n",
    "2. create_visualizations() - load data, create 4-panel overview\n",
    "3. create_visualizations() - create trend analysis plots\n",
    "4. create_visualizations() - generate HTML report\n",
    "5. save_visualization_report() - document all outputs\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EXAMPLE: PREDICTIVE TASK VISUALIZATION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Task: Yield prediction\n",
    "\n",
    "Your workflow:\n",
    "1. load_stage4_result() - get predictions and metrics\n",
    "2. create_visualizations() - actual vs predicted plots\n",
    "3. create_visualizations() - residual analysis\n",
    "4. create_visualizations() - error distribution\n",
    "5. create_visualizations() - generate comprehensive HTML report\n",
    "6. save_visualization_report() - document insights\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Remember:\n",
    "- Create clear, publication-quality visualizations\n",
    "- Tell a story with your charts\n",
    "- Provide actionable insights\n",
    "- Save high-resolution outputs\n",
    "- Generate a comprehensive HTML report\n",
    "- End by calling save_visualization_report()\n",
    "\n",
    "Your success = Beautiful visuals + Meaningful insights + Report saved.\"\"\"\n",
    "\n",
    "# ===========================\n",
    "# LangGraph\n",
    "# ===========================\n",
    "\n",
    "def agent_node(state: MessagesState) -> Dict[str, List[BaseMessage]]:\n",
    "    \"\"\"LLM agent step.\"\"\"\n",
    "    response: AIMessage = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode(TOOLS)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> str:\n",
    "    \"\"\"Route based on tool calls.\"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    if isinstance(last, AIMessage) and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.set_entry_point(\"agent\")\n",
    "builder.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "stage5_app = builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Runner\n",
    "# ===========================\n",
    "\n",
    "def run_stage5_for_result(result_id: str, max_rounds: int = 20, debug: bool = True):\n",
    "    \"\"\"Create visualizations for a Stage 4 result.\"\"\"\n",
    "    \n",
    "    system_msg = SystemMessage(content=STAGE5_SYSTEM_PROMPT)\n",
    "    human_msg = HumanMessage(\n",
    "        content=(\n",
    "            f\"Create comprehensive visualizations for: '{result_id}'\\n\\n\"\n",
    "            f\"Workflow:\\n\"\n",
    "            f\"1. load_stage4_result('{result_id}')\\n\"\n",
    "            f\"2. Understand the task and output files\\n\"\n",
    "            f\"3. FIRST: create_visualizations() to INSPECT data (print columns, head, dtypes)\\n\"\n",
    "            f\"4. SECOND: create_visualizations() to make plots using ACTUAL column names\\n\"\n",
    "            f\"5. Generate HTML report\\n\"\n",
    "            f\"6. save_visualization_report() with all outputs\\n\\n\"\n",
    "            f\"CRITICAL: Never assume column names! Inspect first, then plot.\\n\"\n",
    "            f\"Your success = Data inspected + Beautiful charts + Insights + Report saved.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    state: MessagesState = {\"messages\": [system_msg, human_msg]}\n",
    "\n",
    "    if not debug:\n",
    "        return stage5_app.invoke(state, config={\"configurable\": {\"thread_id\": f\"stage5-{result_id}\"}})\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸš€ STAGE 5: Creating visualizations for {result_id}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    final_state = None\n",
    "    prev_len = 0\n",
    "    round_num = 0\n",
    "\n",
    "    for curr_state in stage5_app.stream(\n",
    "        state,\n",
    "        config={\"configurable\": {\"thread_id\": f\"stage5-{result_id}\"}},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        msgs = curr_state[\"messages\"]\n",
    "        new_msgs = msgs[prev_len:]\n",
    "\n",
    "        for m in new_msgs:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(\"ğŸ’» [SYSTEM]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(m.content[:500] + \"...\" if len(m.content) > 500 else m.content)\n",
    "\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(\"ğŸ‘¤ [USER]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(m.content)\n",
    "\n",
    "            elif isinstance(m, AIMessage):\n",
    "                round_num += 1\n",
    "                print(\"\\n\" + \"â•\" * 80)\n",
    "                print(f\"ğŸ¤– [AGENT - Round {round_num}]\")\n",
    "                print(\"â•\" * 80)\n",
    "\n",
    "                if m.content:\n",
    "                    print(\"\\nğŸ’­ Reasoning:\")\n",
    "                    # ğŸ‘‰ print full reasoning, no truncation\n",
    "                    print(m.content)\n",
    "\n",
    "                if m.tool_calls:\n",
    "                    print(\"\\nğŸ”§ Tool Calls:\")\n",
    "                    for tc in m.tool_calls:\n",
    "                        name = getattr(tc, \"name\", tc.get(\"name\", \"UNKNOWN\"))\n",
    "                        args = getattr(tc, \"args\", tc.get(\"args\", {}))\n",
    "\n",
    "                        print(f\"\\n  ğŸ“Œ {name}\")\n",
    "                        for k, v in args.items():\n",
    "                            # ğŸ‘‰ print full args, especially the generated code\n",
    "                            print(f\"     {k}: {v}\")\n",
    "\n",
    "\n",
    "            elif isinstance(m, ToolMessage):\n",
    "                print(\"\\n\" + \"â”€\" * 80)\n",
    "                print(f\"ğŸ“Š [TOOL: {m.name}]\")\n",
    "                print(\"â”€\" * 80)\n",
    "                print(str(m.content))\n",
    "\n",
    "        prev_len = len(msgs)\n",
    "        final_state = curr_state\n",
    "        \n",
    "        if round_num >= max_rounds:\n",
    "            print(f\"\\nâš ï¸  Reached max rounds ({max_rounds})\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"âœ… Complete - {round_num} rounds\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "        # ğŸ’¾ Save full conversation trace to JSON so you can inspect thoughts later\n",
    "    if final_state is not None:\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        trace_path = STAGE5_OUT_DIR / f\"stage5_trace_{result_id}_{ts}.json\"\n",
    "\n",
    "        try:\n",
    "            trace = messages_to_dict(final_state[\"messages\"])\n",
    "            trace_path.write_text(json.dumps(trace, indent=2))\n",
    "            print(f\"\\nğŸ“ Saved full Stage 5 agent trace to: {trace_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Failed to save trace: {e}\")\n",
    "            \n",
    "    return final_state\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Main\n",
    "# ===========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python stage5_visualizer.py <result_id>\")\n",
    "        print(\"Example: python stage5_visualizer.py execution_PLAN-TSK-002_20250101_120000.json\")\n",
    "        print(\"\\nAvailable results:\")\n",
    "        for result in _list_stage4_results_impl():\n",
    "            print(f\"  - {result}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    result_id = \"execution_PLAN-TSK-002_20251118_082013.json\"\n",
    "    print(f\"\\nğŸ¯ Creating visualizations for: {result_id}\\n\")\n",
    "    \n",
    "    result = run_stage5_for_result(result_id, debug=True)\n",
    "    \n",
    "    # Check for visualization reports\n",
    "    reports = sorted(STAGE5_OUT_DIR.glob(\"visualization_report_*.json\"))\n",
    "    if reports:\n",
    "        latest = reports[-1]\n",
    "        print(f\"\\nâœ… Visualization report: {latest}\")\n",
    "        report_data = json.loads(latest.read_text())\n",
    "        print(\"\\nCreated visualizations:\")\n",
    "        for viz in report_data.get('visualizations', []):\n",
    "            print(f\"  - {viz}\")\n",
    "        if report_data.get('html_report'):\n",
    "            print(f\"\\nHTML Report: {report_data['html_report']}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No visualization report saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4da92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
