{
  "_meta": {
    "timestamp": "2025-12-10T09:42:33.768701",
    "checksum": "a5d95ef110c6c9aa",
    "version": "1.0",
    "stage": "stage3_5a",
    "type": "method_proposal"
  },
  "data": {
    "plan_id": "PLAN-TSK-005",
    "methods_proposed": [
      {
        "method_id": "M1",
        "name": "Linear Regression",
        "category": "baseline",
        "description": "Simple linear regression with one-hot encoding for categorical features",
        "implementation_code": "def predict_linear_regression(train_df, test_df, target_col, date_col, **params):\n    import pandas as pd\n    from sklearn.linear_model import LinearRegression\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n    from sklearn.impute import SimpleImputer\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import mean_absolute_percentage_error\n\n    # Identify categorical and numerical features\n    categorical_features = ['Crop', 'Season']\n    numerical_features = [col for col in train_df.columns if col not in categorical_features + [date_col, target_col]]\n\n    # Create preprocessing pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', SimpleImputer(strategy='mean'), numerical_features),\n            ('cat', Pipeline([\n                ('imputer', SimpleImputer(strategy='most_frequent'),),\n                ('onehot', OneHotEncoder(handle_unknown='ignore'),)\n            ]), categorical_features)\n        ])\n\n    # Create full pipeline\n    model = Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', LinearRegression())\n    ])\n\n    # Train model\n    model.fit(train_df[[date_col] + categorical_features + numerical_features], train_df[target_col])\n\n    # Make predictions\n    predictions = model.predict(test_df[[date_col] + categorical_features + numerical_features])\n\n    # Return predictions\n    return pd.DataFrame({'predicted': predictions}, index=test_df.index)\n",
        "required_libraries": [
          "pandas",
          "sklearn"
        ],
        "hyperparameters": {
          "alpha": 1.0
        },
        "expected_strengths": [
          "Simple and interpretable",
          "Handles both numerical and categorical features"
        ],
        "expected_weaknesses": [
          "Assumes linear relationships",
          "May not capture complex patterns"
        ]
      },
      {
        "method_id": "M2",
        "name": "Ridge Regression",
        "category": "statistical/traditional",
        "description": "Regularized linear regression with L2 penalty",
        "implementation_code": "def predict_ridge_regression(train_df, test_df, target_col, date_col, **params):\n    import pandas as pd\n    from sklearn.linear_model import Ridge\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n    from sklearn.impute import SimpleImputer\n\n    # Identify categorical and numerical features\n    categorical_features = ['Crop', 'Season']\n    numerical_features = [col for col in train_df.columns if col not in categorical_features + [date_col, target_col]]\n\n    # Create preprocessing pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', SimpleImputer(strategy='mean'), numerical_features),\n            ('cat', Pipeline([\n                ('imputer', SimpleImputer(strategy='most_frequent'),),\n                ('onehot', OneHotEncoder(handle_unknown='ignore'),)\n            ]), categorical_features)\n        ])\n\n    # Create full pipeline\n    model = Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', Ridge(alpha=params.get('alpha', 1.0)))\n    ])\n\n    # Train model\n    model.fit(train_df[[date_col] + categorical_features + numerical_features], train_df[target_col])\n\n    # Make predictions\n    predictions = model.predict(test_df[[date_col] + categorical_features + numerical_features])\n\n    # Return predictions\n    return pd.DataFrame({'predicted': predictions}, index=test_df.index)\n",
        "required_libraries": [
          "pandas",
          "sklearn"
        ],
        "hyperparameters": {
          "alpha": 1.0
        },
        "expected_strengths": [
          "Handles multicollinearity",
          "Regularization prevents overfitting"
        ],
        "expected_weaknesses": [
          "Still linear model",
          "May not capture non-linear patterns"
        ]
      },
      {
        "method_id": "M3",
        "name": "Random Forest Regressor",
        "category": "ml",
        "description": "Ensemble of decision trees for non-linear patterns",
        "implementation_code": "def predict_random_forest(train_df, test_df, target_col, date_col, **params):\n    import pandas as pd\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n    from sklearn.impute import SimpleImputer\n\n    # Identify categorical and numerical features\n    categorical_features = ['Crop', 'Season']\n    numerical_features = [col for col in train_df.columns if col not in categorical_features + [date_col, target_col]]\n\n    # Create preprocessing pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', SimpleImputer(strategy='mean'), numerical_features),\n            ('cat', Pipeline([\n                ('imputer', SimpleImputer(strategy='most_frequent'),),\n                ('onehot', OneHotEncoder(handle_unknown='ignore'),)\n            ]), categorical_features)\n        ])\n\n    # Create full pipeline\n    model = Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', RandomForestRegressor(\n            n_estimators=params.get('n_estimators', 100),\n            max_depth=params.get('max_depth', None),\n            random_state=42\n        ))\n    ])\n\n    # Train model\n    model.fit(train_df[[date_col] + categorical_features + numerical_features], train_df[target_col])\n\n    # Make predictions\n    predictions = model.predict(test_df[[date_col] + categorical_features + numerical_features])\n\n    # Return predictions\n    return pd.DataFrame({'predicted': predictions}, index=test_df.index)\n",
        "required_libraries": [
          "pandas",
          "sklearn"
        ],
        "hyperparameters": {
          "n_estimators": 100,
          "max_depth": null
        },
        "expected_strengths": [
          "Handles non-linear relationships",
          "Robust to overfitting",
          "Captures feature interactions"
        ],
        "expected_weaknesses": [
          "Less interpretable",
          "Computationally intensive"
        ]
      }
    ],
    "data_split_strategy": {
      "strategy_type": "temporal_row",
      "date_column": "period",
      "target_column": "Price",
      "train_period": "2012-2023",
      "validation_period": "2024",
      "test_period": "2025"
    },
    "evaluation_metrics": [
      "mae",
      "rmse",
      "r2",
      "mape"
    ],
    "date_column": "period",
    "target_column": "Price"
  }
}