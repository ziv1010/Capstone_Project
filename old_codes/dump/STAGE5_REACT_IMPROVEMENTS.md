# Stage 5 Visualization Improvements: ReAct Framework

## Overview

Stage 5 has been enhanced with a **ReAct (Reasoning + Acting) framework** that makes the visualization agent more thoughtful, transparent, and capable of creating insightful plots that clearly distinguish between given data and predicted data.

## What Changed

### 1. New Tools Added

#### `analyze_data_columns(parquet_path: str)`
Analyzes the structure of Stage 4 output to categorize columns:
- **Given/Original Data**: Columns from the input dataset
- **Predicted/Model Outputs**: Columns generated by the model (predictions, residuals, etc.)
- **Engineered Features**: Derived features (lagged values, growth rates, etc.)
- **Temporal Columns**: Date/time related columns
- **Categorical Columns**: Grouping variables

**Purpose**: Helps the agent understand "what we had" vs "what we predicted" before making any plots.

#### `plan_visualization(thought, plot_type, columns_to_use, purpose, why_this_plot)`
Documents the agent's reasoning **before** creating each plot:
- What has been learned from the data analysis
- What type of plot will be created
- Which columns will be used
- The purpose of the plot
- Justification for the choices

**Purpose**: Makes the agent's decision-making process transparent and ensures thoughtful plot selection.

#### `create_plot_with_explanation(code, plot_number, plot_title, what_it_shows, what_was_given, what_was_predicted, key_insights)`
Creates a plot while documenting comprehensive explanations:
- What the visualization displays
- Which data was **given** (original input)
- Which data was **predicted** (model output)
- Key insights and takeaways

**Purpose**: Ensures every plot comes with complete documentation and clear distinction between input and output data.

### 2. Updated System Prompt

The Stage 5 agent now follows a **mandatory 4-phase ReAct workflow**:

#### Phase 1: ANALYZE
- List and load Stage 4 results
- Understand the original task context
- Use `analyze_data_columns()` to categorize all columns
- Identify what data was given vs predicted

#### Phase 2: REASON (Plan)
- For each intended plot:
  - Use `plan_visualization()` to document reasoning
  - Explain what story the plot tells
  - Justify column and plot type selection
  - Describe how it shows given vs predicted distinction

#### Phase 3: ACT (Execute)
- For each plot:
  - Use `create_plot_with_explanation()`
  - Write professional plotting code
  - Provide comprehensive explanations
  - Clearly distinguish given from predicted data visually (colors, markers, labels)

#### Phase 4: COMPLETE
- Call `save_visualization_report()` with:
  - All plot file paths
  - Summary of insights
  - Overall narrative

### 3. Key Requirements

The agent **must** create visualizations that:

✅ Clearly show GIVEN vs PREDICTED data using different visual styles
✅ Demonstrate the work done (transformations, predictions)
✅ Reveal insights about model performance
✅ Are publication-quality (clear labels, titles, legends)
✅ Tell a coherent story about the analysis pipeline

### 4. Required Plot Types (Adapted to Data)

1. **Predictions vs Actuals**: Compare model outputs to ground truth
2. **Residual Analysis**: Show prediction errors and patterns
3. **Feature Importance**: Highlight which features contributed most
4. **Temporal Trends**: Show how values change over time (if applicable)
5. **Categorical Breakdowns**: Compare across groups (if applicable)

## Benefits

### 1. **Transparency**
Every plot comes with:
- Reasoning for why it was created
- Explanation of what it shows
- Clear documentation of data sources

### 2. **Clear Distinction**
Plots explicitly distinguish:
- What data was provided as input (given)
- What data was generated by the model (predicted)

### 3. **Insightful Analysis**
The agent:
- Analyzes data structure before plotting
- Plans visualizations strategically
- Creates plots that reveal meaningful patterns

### 4. **Reproducibility**
Each plot includes:
- Complete code
- Detailed explanations
- Context about the analysis

## Example Workflow

```python
# Phase 1: Analyze
analyze_data_columns("results_PLAN-TSK-002.parquet")
# Output: Identifies 'Production-2023-24' as given, 'predicted' as model output

# Phase 2: Plan
plan_visualization(
    thought="Need to show prediction accuracy across all crops and seasons",
    plot_type="scatter",
    columns_to_use=["Production-2023-24", "predicted"],
    purpose="Compare model predictions against actual production values",
    why_this_plot="Scatter plot with 45-degree line shows perfect prediction (points on line)"
)

# Phase 3: Execute
create_plot_with_explanation(
    code='''
df = load_dataframe("results_PLAN-TSK-002.parquet")
plt.figure(figsize=(10, 6))
plt.scatter(df["Production-2023-24"], df["predicted"], alpha=0.6, label="Predictions")
plt.plot([df["Production-2023-24"].min(), df["Production-2023-24"].max()],
         [df["Production-2023-24"].min(), df["Production-2023-24"].max()],
         'r--', label="Perfect Prediction")
plt.xlabel("Actual Production (2023-24)")
plt.ylabel("Predicted Production")
plt.title("Model Predictions vs Actual Production")
plt.legend()
plt.tight_layout()
plt.savefig(STAGE5_OUT_DIR / f"plot_{plot_number}_predictions_vs_actual.png", dpi=300)
plt.close()
    ''',
    plot_number=1,
    plot_title="Predictions vs Actual Production (2023-24)",
    what_it_shows="Scatter plot comparing model predictions to actual production values for all crops",
    what_was_given="Production-2023-24 column containing actual production values from the original dataset",
    what_was_predicted="'predicted' column generated by the Linear Regression model trained on 2020-2023 data",
    key_insights="Perfect predictions achieved (RMSE=0.00); all points lie on the 45-degree reference line"
)

# Phase 4: Save Report
save_visualization_report({
    "plan_id": "PLAN-TSK-002",
    "visualizations": ["plot_1_predictions_vs_actual.png", ...],
    "insights": ["Perfect prediction accuracy achieved", ...],
    ...
})
```

## Testing

Run the test script:

```bash
cd /scratch/ziv_baretto/llmserve/final_code
micromamba activate llm
python test_stage5_react.py PLAN-TSK-002
```

This will:
1. Run Stage 5 on existing results
2. Show the ReAct workflow in action
3. Generate plots with explanations
4. Create a comprehensive report

## Output Files

After running Stage 5, check the output directory:

```
/scratch/ziv_baretto/llmserve/final_code/output/stage5_out/
├── plot_1_*.png                    # Visualization 1
├── plot_1_explanation.txt          # Explanation for plot 1
├── plot_2_*.png                    # Visualization 2
├── plot_2_explanation.txt          # Explanation for plot 2
├── ...
└── visualization_report_*.json    # Complete report
```

Each `.txt` file contains:
- What the plot shows
- What data was given (from original dataset)
- What data was predicted (from model)
- Key insights

## Future Enhancements

Potential improvements:
- Interactive visualizations (Plotly dashboards)
- Automated HTML report generation
- Statistical significance testing for comparisons
- Automated plot selection based on data characteristics
- Integration with external visualization tools

## Summary

The ReAct framework makes Stage 5:
- **More thoughtful**: Analyzes before plotting
- **More transparent**: Documents reasoning and decisions
- **More useful**: Clearly shows given vs predicted data
- **More insightful**: Creates plots that tell a story

The agent now operates like a data scientist who:
1. Examines the data carefully
2. Plans visualizations strategically
3. Creates professional plots with clear explanations
4. Documents everything for reproducibility
