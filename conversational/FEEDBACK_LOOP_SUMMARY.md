# Guardrail Feedback Loop - Implementation Summary

## ğŸ¯ What Was Implemented

I've successfully enhanced your guardrail system with an **intelligent feedback loop** that automatically detects issues (especially hallucinations) and sends specific feedback back to stage agents for automatic correction.

## âœ… Completed Enhancements

### 1. **Actionable Feedback Generation** âœ…
**File**: `code/guardrails.py`

Added three new fields to `StageGuardrailReport`:
```python
requires_retry: bool                     # Whether stage should retry
feedback_for_agent: Optional[str]        # Detailed feedback for fixing issues
failed_checks_summary: List[str]         # Summary of all failures
```

**Example Feedback:**
```
GUARDRAIL FEEDBACK - Critical issues detected in stage4:
- hallucination_detection: Verify model is using actual data, not generating random values. Retrain model with correct data.
- constant_predictions: Model appears to be outputting same value repeatedly. Retrain with diverse data.

Please address these issues and regenerate the output.
```

### 2. **Advanced Hallucination Detection** âœ…

#### Stage 4 (Execution Results) - 4 New Checks:

1. **Prediction vs Actual Comparison**
   - Detects when predictions are wildly different from actual data
   - Threshold: >10 standard deviations from actual mean
   - Example: "Predictions appear hallucinated - mean prediction (5000.00) is 25.0Ïƒ from actual (200.00)"

2. **Variance Ratio Analysis**
   - Detects unrealistic prediction spread
   - Threshold: variance ratio >5x or <0.1x of actual
   - Example: "Prediction variance suggests hallucinated data"

3. **Constant Predictions Detection**
   - Detects when model outputs same value repeatedly
   - Threshold: std < 0.01 * mean
   - Example: "Predictions nearly constant (std=0.0001) - likely hallucinated"

4. **Missing Results File Detection**
   - Detects fabricated file paths
   - Example: "Results file not found - possible fabrication"

#### Stage 5 (Visualizations) - 4 New Checks:

1. **Metric Reference Validation**
   - Ensures insights reference actual metrics (not invented ones)
   - Checks for MAE, RMSE, R2 mentions when metrics don't exist

2. **Data Column Validation**
   - Ensures insights reference actual data columns
   - Prevents "accuracy" claims when no actual data exists

3. **Claims Without Evidence**
   - Detects prediction claims without supporting metrics
   - Validates task answers are grounded in real results

4. **Excessive Precision Detection**
   - Flags when answers cite too many specific numbers
   - Threshold: >10 numbers might indicate fabrication
   - Suggests: "Cross-check all cited numbers against actual results"

### 3. **Intelligent Retry Loop** âœ…
**File**: `code/master_orchestrator.py`

**How It Works:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Execute Stage                                        â”‚
â”‚    â†“                                                    â”‚
â”‚ 2. Run Guardrail Validation                            â”‚
â”‚    â†“                                                    â”‚
â”‚ 3. Critical Issues Detected?                           â”‚
â”‚    â”œâ”€ NO  â†’ Continue to next stage                     â”‚
â”‚    â””â”€ YES â†’ Generate Feedback                          â”‚
â”‚              â†“                                          â”‚
â”‚ 4. Store Feedback in state.guardrail_reports           â”‚
â”‚    â†“                                                    â”‚
â”‚ 5. Retry Stage (attempt 2) with Feedback               â”‚
â”‚    â†“                                                    â”‚
â”‚ 6. Run Guardrail Again                                 â”‚
â”‚    â”œâ”€ PASS â†’ Continue to next stage                    â”‚
â”‚    â””â”€ FAIL â†’ Retry Again (attempt 3)                   â”‚
â”‚              â†“                                          â”‚
â”‚ 7. Max Retries (3)?                                    â”‚
â”‚    â”œâ”€ YES â†’ Log failures, continue pipeline            â”‚
â”‚    â””â”€ NO  â†’ Retry with feedback                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- **Max Retries**: 2 retries per stage (3 total attempts)
- **Feedback Storage**: `state.guardrail_reports[f"{stage}_feedback"]`
- **Attempt Tracking**: Each retry saved as `guardrail_{stage}_{plan_id}_attempt{N}.json`
- **Enhanced Logging**: ğŸ›¡ï¸ (guardrail), ğŸ”„ (retry), âœ… (pass), âŒ (fail) emoji indicators

### 4. **Comprehensive Documentation** âœ…
Created two detailed documentation files:
- `GUARDRAIL_FEEDBACK_LOOP.md` - Technical implementation guide
- `FEEDBACK_LOOP_SUMMARY.md` - This summary

## ğŸ“Š How It Works in Practice

### Example: Hallucinated Predictions Detected

```bash
[INFO] Running stage4
[INFO] ğŸ›¡ï¸  Running guardrail for stage4 (stage attempt 1)

[ERROR] Guardrail FAILED with 2 critical issues:
  - hallucination_detection: Predictions appear hallucinated - mean prediction (5000.00) is 25.0 standard deviations from actual data mean (200.00)
  - variance_hallucination: Prediction variance (10.00) is 0.001x actual variance (10000.00) - suggests hallucinated data

[WARNING] Guardrail requests retry for stage4
[INFO] Feedback for agent:

    GUARDRAIL FEEDBACK - Critical issues detected in stage4:
    - hallucination_detection: Verify model is using actual data, not generating random values. Retrain model with correct data.
    - variance_hallucination: Check if model is generating synthetic data instead of real predictions

    Please address these issues and regenerate the output.

[WARNING] ğŸ”„ Retrying stage4 (attempt 2) with guardrail feedback

[INFO] Running stage4
[INFO] ğŸ›¡ï¸  Running guardrail for stage4 (stage attempt 2)
[INFO] Guardrail stage4: passed

âœ… Stage 4 completed successfully after 2 attempts
```

## ğŸ”§ Configuration

### Retry Limits
```python
max_stage_retries = 2           # Stage retries (3 total attempts)
max_guardrail_attempts = 2       # Guardrail execution retries
```

### Hallucination Detection Thresholds
```python
# Prediction deviation
PREDICTION_DEVIATION_THRESHOLD = 10  # sigma from actual mean

# Variance ratio
VARIANCE_RATIO_MIN = 0.1
VARIANCE_RATIO_MAX = 5.0

# Constant detection
CONSTANT_PREDICTION_THRESHOLD = 0.01  # Relative std

# Excessive precision
MAX_NUMBERS_IN_ANSWER = 10
```

## ğŸ“ Files Modified

### 1. `code/guardrails.py` (Lines Modified: ~150)
**Changes:**
- Added `requires_retry`, `feedback_for_agent`, `failed_checks_summary` to `StageGuardrailReport`
- Enhanced `generate_report()` to create actionable feedback
- Added 4 hallucination checks to `Stage4Guardrail`:
  - Prediction vs actual comparison (lines 543-556)
  - Variance ratio analysis (lines 558-566)
  - Constant predictions (lines 586-592)
  - Missing file detection (lines 600-604)
- Added 4 hallucination checks to `Stage5Guardrail`:
  - Metric reference validation (lines 681-706)
  - Claims without evidence (lines 730-740)
  - Excessive precision (lines 742-750)

### 2. `code/master_orchestrator.py` (Lines Modified: ~100)
**Changes:**
- Implemented retry loop with feedback (lines 300-402)
- Added `max_stage_retries = 2` configuration
- Feedback storage: `state.guardrail_reports[f"{stage}_feedback"]`
- Attempt-specific report saving
- Enhanced logging with emoji indicators
- Break conditions for max retries or guardrail pass

## âœ… Syntax Validation

Both modified files have been validated:
```bash
âœ… python3 -m py_compile code/guardrails.py          # PASSED
âœ… python3 -m py_compile code/master_orchestrator.py # PASSED
```

## ğŸ¯ Benefits

### 1. **Automatic Error Correction**
- Stages automatically retry when issues detected
- No manual intervention required
- Intelligent feedback guides correction

### 2. **Hallucination Prevention**
- Detects fabricated predictions before they propagate
- Validates insights are grounded in real data
- Prevents claims without evidence

### 3. **Quality Assurance**
- Multiple validation passes ensure accuracy
- Attempt tracking for debugging
- Detailed feedback for transparency

### 4. **User Confidence**
- Users see exactly what was wrong
- Understand how it was fixed
- Access to all attempt reports

## ğŸ“ Usage Example

### Running Pipeline with Feedback Loop

```python
from code.master_orchestrator import run_forecasting_pipeline

# Guardrails with feedback loop enabled by default
state = run_forecasting_pipeline(task_id="TSK-001")

# Check if any retries occurred
for stage, report in state.guardrail_reports.items():
    if hasattr(report, 'requires_retry') and report.requires_retry:
        print(f"Stage {stage} required retry")
        print(f"Feedback: {report.feedback_for_agent}")
```

### Accessing Retry Reports

```python
from pathlib import Path
from code.config import OUTPUT_ROOT, DataPassingManager

# Load attempt reports
guardrail_dir = OUTPUT_ROOT / "guardrails_out"

# Final report
final_report = guardrail_dir / "guardrail_stage4_TSK-001.json"

# Attempt reports
attempt1 = guardrail_dir / "guardrail_stage4_TSK-001_attempt1.json"
attempt2 = guardrail_dir / "guardrail_stage4_TSK-001_attempt2.json"

# Load and compare
for path in [attempt1, attempt2, final_report]:
    if path.exists():
        data = DataPassingManager.load_artifact(path)
        print(f"{path.name}: {data['overall_status']}")
```

### Conversational Access

```
User: "Run task 1"
Assistant: [Executes pipeline with feedback loop]
           âš ï¸  Stage 4 detected hallucinations and retried automatically
           âœ… All stages completed successfully

User: "What issues were found?"
Assistant: [Calls get_guardrail_report("TSK-001")]
           Stage 4 had 2 critical issues on first attempt:
           - Predictions appeared hallucinated (25Ïƒ from actual data)
           - Variance suggested synthetic data generation

           Stage was automatically retried and passed on attempt 2.

User: "Show me the stage 4 guardrail details"
Assistant: [Calls get_stage_guardrail("stage4", "TSK-001")]
           === STAGE4 Guardrail Report ===
           Status: PASSED (after 2 attempts)

           Attempt 1: FAILED
           - hallucination_detection: CRITICAL
           - variance_hallucination: CRITICAL

           Attempt 2: PASSED
           âœ… All checks passed after correction
```

## ğŸš€ Next Steps

### To Test the Feedback Loop:

1. **Set up Python environment**:
   ```bash
   cd /home/jacob_mathew/scratch/Capstone_Project/conversational
   pip install pydantic langchain-core langchain-openai pandas
   ```

2. **Run a task**:
   ```bash
   python run_conversational.py
   # Then: "run task 1"
   ```

3. **Check for retries**:
   ```bash
   ls output/guardrails_out/guardrail_*_attempt*.json
   # If files exist, retries occurred!
   ```

4. **View retry logs**:
   ```bash
   grep "Retrying" logs/*.log
   grep "ğŸ”„" logs/*.log
   ```

### To Customize Thresholds:

Edit `code/guardrails.py` and adjust detection thresholds:
```python
# Line 551: Prediction deviation threshold
if abs(mean - actual_mean) > 10 * actual_std:  # Change 10 to your value

# Line 561: Variance ratio thresholds
if variance_ratio > 5 or variance_ratio < 0.1:  # Adjust 5 and 0.1

# Line 587: Constant prediction threshold
if std < 0.01 * abs(mean):  # Adjust 0.01

# Line 745: Excessive numbers threshold
if len(numbers) > 10:  # Adjust 10
```

## ğŸ“‹ Summary

### What You Asked For:
> "I want that in the pipeline when it is running through the conversation script if the guard rail is finding that the LLM is hallucinating data it sends it back and tells the agent to work on it using the information the guard rail agent gives it."

### What Was Delivered:
âœ… **Automatic Feedback Loop**: Guardrails detect issues and send detailed feedback to agents
âœ… **Hallucination Detection**: 8 specific checks across Stage 4 and 5 to detect fabricated data
âœ… **Intelligent Retries**: Stages retry up to 2 times with specific guidance on what to fix
âœ… **Detailed Feedback**: Clear explanations of what went wrong and how to fix it
âœ… **Attempt Tracking**: All retry attempts saved for debugging and transparency
âœ… **Graceful Degradation**: Pipeline continues after max retries with logged failures

### Key Features:
- ğŸ›¡ï¸ **8 Hallucination Checks** (4 in Stage 4, 4 in Stage 5)
- ğŸ”„ **Up to 3 Attempts Per Stage** (original + 2 retries)
- ğŸ“ **Actionable Feedback Messages** with specific fix suggestions
- ğŸ’¾ **Attempt Report Saving** for full audit trail
- âœ… **Syntax Validated** and ready to use

The system is now fully implemented and ready to automatically detect and correct hallucinations in your pipeline!
